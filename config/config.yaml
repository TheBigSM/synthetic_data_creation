# Configuration for LLM APIs
# Updated with cost-effective options (July 2025 pricing)
llm:
  # Ultra-low-cost option (recommended for large-scale)
  together_llama4_scout:
    model: "meta-llama/Llama-4-Scout-17B-16E-Instruct-FP8"
    temperature: 0.7
    max_tokens: 1000
    base_url: "https://api.together.xyz/v1"
    cost_per_1k_tokens: 0.00385  # $0.18 input + $0.59 output averaged
    
  # Balanced cost/performance option (use this one with your $1 budget)
  together_llama4_maverick:
    model: "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8"
    temperature: 0.7
    max_tokens: 1000
    base_url: "https://api.together.xyz/v1"
    cost_per_1k_tokens: 0.0056   # $0.27 input + $0.85 output averaged
    
  openai_nano:
    model: "gpt-4.1-nano"
    temperature: 0.7
    max_tokens: 1000
    cost_per_1k_tokens: 0.0005  # $0.10 input + $0.40 output averaged
    
  # Balanced cost/quality options
  openai_mini:
    model: "gpt-4o-mini"
    temperature: 0.7
    max_tokens: 1000
    cost_per_1k_tokens: 0.0007  # $0.15 input + $0.60 output averaged
    
  # Higher quality options
  openai_standard:
    model: "gpt-4.1-mini"
    temperature: 0.7
    max_tokens: 1000
    cost_per_1k_tokens: 0.0208  # $0.40 input + $1.60 output averaged
    
  # Premium options (for critical quality needs)
  openai_premium:
    model: "gpt-4.1"
    temperature: 0.7
    max_tokens: 1000
    cost_per_1k_tokens: 0.050   # $2.00 input + $8.00 output averaged

# Default provider selection (change this to switch providers easily)
default_llm_provider: "together_llama4_maverick"  # Options: together_llama4_scout, together_llama4_maverick, openai_nano, openai_mini, etc.

# Data generation settings
data_generation:
  batch_size: 10
  facts_per_article: 3
  synthetic_ratio: 1.0  # 1:1 ratio of synthetic to real data
  
# Classification model settings
classification:
  test_size: 0.2
  validation_size: 0.1
  random_state: 42
  
  tokenization:
    max_length: 512
    padding: true
    truncation: true
  
  models:
    traditional_ml:
      - "svm"
      - "random_forest"
      - "naive_bayes"
      - "logistic_regression"
    
    deep_learning:
      - "lstm"
      - "bilstm"
      - "bert-base-uncased"
      - "roberta-base"

# Training settings
training:
  epochs: 10
  batch_size: 32
  learning_rate: 0.0001
  early_stopping_patience: 3
  
# Evaluation settings
evaluation:
  metrics:
    - "accuracy"
    - "precision"
    - "recall"
    - "f1"
    - "roc_auc"
  
  cross_validation_folds: 5
