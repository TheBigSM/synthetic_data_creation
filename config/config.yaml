# Configuration for LLM APIs
# Updated with cost-effective options (July 2025 pricing)
llm:
  # Ultra-low-cost options (recommended for large-scale)
  groq_llama4:
    model: "llama-4-scout"
    temperature: 0.7
    max_tokens: 1000
    base_url: "https://api.groq.com/openai/v1"
    cost_per_1k_tokens: 0.0005  # $0.11 input + $0.34 output averaged
    
  openai_nano:
    model: "gpt-4.1-nano"
    temperature: 0.7
    max_tokens: 1000
    cost_per_1k_tokens: 0.0005  # $0.10 input + $0.40 output averaged
    
  # Balanced cost/quality options
  openai_mini:
    model: "gpt-4o-mini"
    temperature: 0.7
    max_tokens: 1000
    cost_per_1k_tokens: 0.0007  # $0.15 input + $0.60 output averaged
    
  fireworks_llama4:
    model: "llama-4-maverick"
    temperature: 0.7
    max_tokens: 1000
    base_url: "https://api.fireworks.ai/inference/v1"
    cost_per_1k_tokens: 0.0011  # $0.27 input + $0.85 output averaged
    
  # Higher quality options
  anthropic_haiku:
    model: "claude-3-5-haiku-20241022"
    temperature: 0.7
    max_tokens: 1000
    cost_per_1k_tokens: 0.0042  # $0.80 input + $4.00 output averaged
    
  openai_standard:
    model: "gpt-4.1-mini"
    temperature: 0.7
    max_tokens: 1000
    cost_per_1k_tokens: 0.0208  # $0.40 input + $1.60 output averaged
    
  # Premium options (for critical quality needs)
  anthropic_sonnet:
    model: "claude-3-sonnet-4"
    temperature: 0.7
    max_tokens: 1000
    cost_per_1k_tokens: 0.018   # $3.00 input + $15.00 output averaged
    
  openai_premium:
    model: "gpt-4.1"
    temperature: 0.7
    max_tokens: 1000
    cost_per_1k_tokens: 0.050   # $2.00 input + $8.00 output averaged

# Default provider selection (change this to switch providers easily)
default_llm_provider: "groq_llama4"  # Options: groq_llama4, openai_nano, openai_mini, anthropic_haiku, etc.

# Data generation settings
data_generation:
  batch_size: 10
  facts_per_article: 3
  synthetic_ratio: 1.0  # 1:1 ratio of synthetic to real data
  
# Classification model settings
classification:
  test_size: 0.2
  validation_size: 0.1
  random_state: 42
  
  tokenization:
    max_length: 512
    padding: true
    truncation: true
  
  models:
    traditional_ml:
      - "svm"
      - "random_forest"
      - "naive_bayes"
      - "logistic_regression"
    
    deep_learning:
      - "lstm"
      - "bilstm"
      - "bert-base-uncased"
      - "roberta-base"

# Training settings
training:
  epochs: 10
  batch_size: 32
  learning_rate: 0.0001
  early_stopping_patience: 3
  
# Evaluation settings
evaluation:
  metrics:
    - "accuracy"
    - "precision"
    - "recall"
    - "f1"
    - "roc_auc"
  
  cross_validation_folds: 5
