# Configuration for LLM APIs
llm:
  openai:
    model: "gpt-3.5-turbo"
    temperature: 0.7
    max_tokens: 1000
  
  anthropic:
    model: "claude-3-haiku-20240307"
    temperature: 0.7
    max_tokens: 1000

# Data generation settings
data_generation:
  batch_size: 10
  facts_per_article: 3
  synthetic_ratio: 1.0  # 1:1 ratio of synthetic to real data
  
# Classification model settings
classification:
  test_size: 0.2
  validation_size: 0.1
  random_state: 42
  
  tokenization:
    max_length: 512
    padding: true
    truncation: true
  
  models:
    traditional_ml:
      - "svm"
      - "random_forest"
      - "naive_bayes"
      - "logistic_regression"
    
    deep_learning:
      - "lstm"
      - "bilstm"
      - "bert-base-uncased"
      - "roberta-base"

# Training settings
training:
  epochs: 10
  batch_size: 32
  learning_rate: 0.0001
  early_stopping_patience: 3
  
# Evaluation settings
evaluation:
  metrics:
    - "accuracy"
    - "precision"
    - "recall"
    - "f1"
    - "roc_auc"
  
  cross_validation_folds: 5
