# Configuration for LLM APIs
llm:
  openai:
    model: "gpt-4o-mini"  # Updated to cost-effective model ($0.15/$0.60 per 1M tokens)
    temperature: 0.7
    max_tokens: 1000
  
  anthropic:
    model: "claude-3-5-haiku-20241022"  # Updated to Haiku 3.5 ($0.80/$4.00 per 1M tokens)
    temperature: 0.7
    max_tokens: 1000
  
  llama4:
    model: "llama-4-scout"  # Groq's ultra-low-cost option ($0.11/$0.34 per 1M tokens)
    temperature: 0.7
    max_tokens: 1000
    base_url: "https://api.groq.com/openai/v1"  # Groq endpoint for Llama 4

# Data generation settings
data_generation:
  batch_size: 10
  facts_per_article: 3
  synthetic_ratio: 1.0  # 1:1 ratio of synthetic to real data
  
# Classification model settings
classification:
  test_size: 0.2
  validation_size: 0.1
  random_state: 42
  
  tokenization:
    max_length: 512
    padding: true
    truncation: true
  
  models:
    traditional_ml:
      - "svm"
      - "random_forest"
      - "naive_bayes"
      - "logistic_regression"
    
    deep_learning:
      - "lstm"
      - "bilstm"
      - "bert-base-uncased"
      - "roberta-base"

# Training settings
training:
  epochs: 10
  batch_size: 32
  learning_rate: 0.0001
  early_stopping_patience: 3
  
# Evaluation settings
evaluation:
  metrics:
    - "accuracy"
    - "precision"
    - "recall"
    - "f1"
    - "roc_auc"
  
  cross_validation_folds: 5
