{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae468aa2",
   "metadata": {},
   "source": [
    "# COVID Synthetic Data Generation - Full Methodology\n",
    "\n",
    "This notebook implements the complete 4-step methodology for synthetic data generation:\n",
    "\n",
    "1. **Data Collection** (already done)\n",
    "2. **Fact Characterization** - Define fact types with name, description, and examples\n",
    "3. **Fact Extraction** - Extract structured facts using LLM\n",
    "4. **Fact Manipulation** - Modify facts and generate synthetic articles\n",
    "\n",
    "The output follows the specified JSON format with:\n",
    "- original_article\n",
    "- extracted_facts\n",
    "- modified_facts (before replacing)\n",
    "- modified_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333498b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import custom modules\n",
    "from src.data_generation.pipeline import SyntheticDataPipeline, run_demo_pipeline\n",
    "from src.data_generation.fact_schemas import get_fact_schema, display_fact_schema, COVID_FACT_SCHEMA\n",
    "from src.utils.evaluation import EvaluationManager\n",
    "from src.utils.data_utils import load_config\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2362bb89",
   "metadata": {},
   "source": [
    "## Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ba2aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check API keys and configuration\n",
    "config = load_config()\n",
    "\n",
    "openai_key = os.getenv('OPENAI_API_KEY')\n",
    "print(f\"OpenAI API Key available: {openai_key is not None}\")\n",
    "\n",
    "if not openai_key:\n",
    "    print(\"\\n‚ö†Ô∏è  Warning: No OpenAI API key found.\")\n",
    "    print(\"Please set up your .env file with OPENAI_API_KEY\")\n",
    "    print(\"Copy .env.example to .env and add your API key\")\n",
    "else:\n",
    "    print(\"‚úÖ API key configured - ready to proceed\")\n",
    "\n",
    "print(\"\\nConfiguration loaded:\")\n",
    "print(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4c99d0",
   "metadata": {},
   "source": [
    "## Step 2: Fact Characterization\n",
    "\n",
    "Define COVID-specific fact types with:\n",
    "- Name of fact\n",
    "- Description of fact  \n",
    "- Common examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8917cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the predefined COVID fact schema\n",
    "print(\"COVID-SPECIFIC FACT CHARACTERIZATION SCHEMA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "fact_schema = get_fact_schema(\"news\")\n",
    "display_fact_schema(fact_schema)\n",
    "\n",
    "print(f\"\\nTotal fact types defined: {len(fact_schema)}\")\n",
    "print(\"\\nThis schema will be used to extract structured facts from COVID articles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a893a43",
   "metadata": {},
   "source": [
    "## Demo Pipeline - Process 10 Articles\n",
    "\n",
    "Start with small batch as per methodology (10 articles, then 100, then full dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8f7666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run demo pipeline with 10 articles\n",
    "print(\"RUNNING DEMO PIPELINE - 10 ARTICLES\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "if openai_key:\n",
    "    try:\n",
    "        # This will create sample data and run the full pipeline\n",
    "        pipeline = run_demo_pipeline(num_articles=10, content_type=\"news\")\n",
    "        \n",
    "        print(\"\\n‚úÖ Demo pipeline completed successfully!\")\n",
    "        \n",
    "        # Get results for analysis\n",
    "        results = pipeline.results\n",
    "        print(f\"Generated {len(results)} synthetic article pairs\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error running pipeline: {e}\")\n",
    "        print(\"Please check your API key and configuration\")\n",
    "        results = []\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping pipeline execution - no API key configured\")\n",
    "    print(\"Please add your OpenAI API key to .env file to run the pipeline\")\n",
    "    results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1208310",
   "metadata": {},
   "source": [
    "## Results Analysis - Structured Output Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb6b785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results in the required JSON format\n",
    "if results:\n",
    "    print(\"SYNTHETIC DATA GENERATION RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Show first 2 results in detail\n",
    "    for i, result in enumerate(results[:2]):\n",
    "        print(f\"\\nüìÑ ARTICLE {i + 1}\")\n",
    "        print(\"-\"*30)\n",
    "        \n",
    "        print(\"\\nüîπ ORIGINAL ARTICLE:\")\n",
    "        print(f\"{result.original_article[:200]}{'...' if len(result.original_article) > 200 else ''}\")\n",
    "        \n",
    "        print(\"\\nüîπ EXTRACTED FACTS:\")\n",
    "        for j, fact in enumerate(result.extracted_facts, 1):\n",
    "            print(f\"  {j}. {fact.get('name_of_fact', 'Unknown')}: {fact.get('specific_data', 'N/A')}\")\n",
    "            print(f\"     Description: {fact.get('description_of_fact', 'N/A')}\")\n",
    "        \n",
    "        print(\"\\nüîπ MODIFIED FACTS (before replacement):\")\n",
    "        for j, fact in enumerate(result.modified_facts, 1):\n",
    "            print(f\"  {j}. {fact.get('name_of_fact', 'Unknown')}: {fact.get('specific_data', 'N/A')}\")\n",
    "        \n",
    "        print(\"\\nüîπ MODIFIED ARTICLE:\")\n",
    "        print(f\"{result.modified_article[:200]}{'...' if len(result.modified_article) > 200 else ''}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "    \n",
    "    if len(results) > 2:\n",
    "        print(f\"\\n... and {len(results) - 2} more results\")\n",
    "        \n",
    "else:\n",
    "    print(\"No results to display. Run the pipeline first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d491ca",
   "metadata": {},
   "source": [
    "## Quality Assessment - Structured Facts Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2032d0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the quality and structure of extracted/modified facts\n",
    "if results:\n",
    "    print(\"FACT EXTRACTION AND MODIFICATION ANALYSIS\")\n",
    "    print(\"=\"*45)\n",
    "    \n",
    "    # Collect statistics\n",
    "    fact_stats = {\n",
    "        'total_articles': len(results),\n",
    "        'total_facts_extracted': 0,\n",
    "        'total_facts_modified': 0,\n",
    "        'fact_types_found': {},\n",
    "        'avg_facts_per_article': 0\n",
    "    }\n",
    "    \n",
    "    all_extracted_facts = []\n",
    "    all_modified_facts = []\n",
    "    \n",
    "    for result in results:\n",
    "        extracted_count = len(result.extracted_facts)\n",
    "        modified_count = len(result.modified_facts)\n",
    "        \n",
    "        fact_stats['total_facts_extracted'] += extracted_count\n",
    "        fact_stats['total_facts_modified'] += modified_count\n",
    "        \n",
    "        all_extracted_facts.extend(result.extracted_facts)\n",
    "        all_modified_facts.extend(result.modified_facts)\n",
    "        \n",
    "        # Count fact types\n",
    "        for fact in result.extracted_facts:\n",
    "            fact_type = fact.get('name_of_fact', 'Unknown')\n",
    "            fact_stats['fact_types_found'][fact_type] = fact_stats['fact_types_found'].get(fact_type, 0) + 1\n",
    "    \n",
    "    fact_stats['avg_facts_per_article'] = fact_stats['total_facts_extracted'] / len(results)\n",
    "    \n",
    "    # Display statistics\n",
    "    print(f\"Total articles processed: {fact_stats['total_articles']}\")\n",
    "    print(f\"Total facts extracted: {fact_stats['total_facts_extracted']}\")\n",
    "    print(f\"Total facts modified: {fact_stats['total_facts_modified']}\")\n",
    "    print(f\"Average facts per article: {fact_stats['avg_facts_per_article']:.1f}\")\n",
    "    \n",
    "    print(\"\\nFact types found:\")\n",
    "    for fact_type, count in sorted(fact_stats['fact_types_found'].items(), key=lambda x: x[1], reverse=True):\n",
    "        percentage = (count / fact_stats['total_facts_extracted']) * 100\n",
    "        print(f\"  {fact_type}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Visualization\n",
    "    if len(fact_stats['fact_types_found']) > 0:\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        # Fact types distribution\n",
    "        plt.subplot(1, 2, 1)\n",
    "        fact_types = list(fact_stats['fact_types_found'].keys())\n",
    "        fact_counts = list(fact_stats['fact_types_found'].values())\n",
    "        \n",
    "        plt.bar(range(len(fact_types)), fact_counts)\n",
    "        plt.xlabel('Fact Types')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title('Distribution of Extracted Fact Types')\n",
    "        plt.xticks(range(len(fact_types)), fact_types, rotation=45, ha='right')\n",
    "        \n",
    "        # Facts per article distribution\n",
    "        plt.subplot(1, 2, 2)\n",
    "        facts_per_article = [len(result.extracted_facts) for result in results]\n",
    "        plt.hist(facts_per_article, bins=max(1, len(set(facts_per_article))), alpha=0.7, edgecolor='black')\n",
    "        plt.xlabel('Number of Facts per Article')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Facts per Article Distribution')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "else:\n",
    "    print(\"No results available for analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7900fc6a",
   "metadata": {},
   "source": [
    "## Manual Evaluation Setup\n",
    "\n",
    "Create template for manual annotation by 3+ annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519123a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluation template for manual annotation\n",
    "if results:\n",
    "    print(\"CREATING MANUAL EVALUATION TEMPLATE\")\n",
    "    print(\"=\"*38)\n",
    "    \n",
    "    # Create evaluation manager and template\n",
    "    eval_manager = EvaluationManager()\n",
    "    \n",
    "    # Convert results to evaluation format\n",
    "    eval_data = []\n",
    "    for result in results:\n",
    "        eval_data.append({\n",
    "            'original_article': result.original_article,\n",
    "            'extracted_facts': result.extracted_facts,\n",
    "            'modified_facts': result.modified_facts,\n",
    "            'modified_article': result.modified_article\n",
    "        })\n",
    "    \n",
    "    # Create template\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    template_file = f\"results/evaluation_template_{timestamp}.csv\"\n",
    "    \n",
    "    # Ensure results directory exists\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    \n",
    "    eval_manager.create_annotation_template(eval_data, template_file)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Evaluation template created: {template_file}\")\n",
    "    print(\"\\nüìã Next steps for manual evaluation:\")\n",
    "    print(\"1. Share the CSV file with 3+ annotators\")\n",
    "    print(\"2. Each annotator should rate:\")\n",
    "    print(\"   - fact_extraction_quality: appropriate/inappropriate/in-between\")\n",
    "    print(\"   - fact_modification_quality: appropriate/inappropriate/in-between\")\n",
    "    print(\"   - synthetic_article_quality: appropriate/inappropriate/in-between\")\n",
    "    print(\"3. Collect completed annotations\")\n",
    "    print(\"4. Use the evaluation analysis tools to calculate agreement\")\n",
    "    \n",
    "else:\n",
    "    print(\"No results available for evaluation template creation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf35da4",
   "metadata": {},
   "source": [
    "## Automatic Evaluation Metrics\n",
    "\n",
    "Calculate correctness, coherence, and dissimilarity automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f71a257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run automatic evaluation\n",
    "if results:\n",
    "    print(\"AUTOMATIC EVALUATION METRICS\")\n",
    "    print(\"=\"*30)\n",
    "    \n",
    "    eval_manager = EvaluationManager()\n",
    "    automatic_evaluations = []\n",
    "    \n",
    "    for result in results:\n",
    "        auto_eval = eval_manager.calculate_automatic_metrics(\n",
    "            original_text=result.original_article,\n",
    "            synthetic_text=result.modified_article,\n",
    "            extracted_facts=result.extracted_facts,\n",
    "            modified_facts=result.modified_facts\n",
    "        )\n",
    "        automatic_evaluations.append(auto_eval)\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    correctness_scores = [eval.correctness for eval in automatic_evaluations]\n",
    "    coherence_scores = [eval.coherence for eval in automatic_evaluations]\n",
    "    dissimilarity_scores = [eval.dissimilarity for eval in automatic_evaluations]\n",
    "    \n",
    "    print(f\"\\nüìä SUMMARY STATISTICS (n={len(automatic_evaluations)}):\")\n",
    "    print(f\"\\nCorrectness (fact incorporation):\")\n",
    "    print(f\"  Mean: {np.mean(correctness_scores):.3f} ¬± {np.std(correctness_scores):.3f}\")\n",
    "    print(f\"  Range: [{np.min(correctness_scores):.3f}, {np.max(correctness_scores):.3f}]\")\n",
    "    \n",
    "    print(f\"\\nCoherence (text quality):\")\n",
    "    print(f\"  Mean: {np.mean(coherence_scores):.3f} ¬± {np.std(coherence_scores):.3f}\")\n",
    "    print(f\"  Range: [{np.min(coherence_scores):.3f}, {np.max(coherence_scores):.3f}]\")\n",
    "    \n",
    "    print(f\"\\nDissimilarity (from original):\")\n",
    "    print(f\"  Mean: {np.mean(dissimilarity_scores):.3f} ¬± {np.std(dissimilarity_scores):.3f}\")\n",
    "    print(f\"  Range: [{np.min(dissimilarity_scores):.3f}, {np.max(dissimilarity_scores):.3f}]\")\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Correctness distribution\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.hist(correctness_scores, bins=10, alpha=0.7, edgecolor='black')\n",
    "    plt.axvline(np.mean(correctness_scores), color='red', linestyle='--', \n",
    "                label=f'Mean: {np.mean(correctness_scores):.3f}')\n",
    "    plt.xlabel('Correctness Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Fact Incorporation Correctness')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Coherence distribution\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.hist(coherence_scores, bins=10, alpha=0.7, edgecolor='black')\n",
    "    plt.axvline(np.mean(coherence_scores), color='red', linestyle='--',\n",
    "                label=f'Mean: {np.mean(coherence_scores):.3f}')\n",
    "    plt.xlabel('Coherence Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Text Coherence Quality')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Dissimilarity distribution\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.hist(dissimilarity_scores, bins=10, alpha=0.7, edgecolor='black')\n",
    "    plt.axvline(np.mean(dissimilarity_scores), color='red', linestyle='--',\n",
    "                label=f'Mean: {np.mean(dissimilarity_scores):.3f}')\n",
    "    plt.xlabel('Dissimilarity Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Dissimilarity from Original')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Quality assessment\n",
    "    print(\"\\nüéØ QUALITY ASSESSMENT:\")\n",
    "    \n",
    "    avg_correctness = np.mean(correctness_scores)\n",
    "    if avg_correctness > 0.8:\n",
    "        print(f\"‚úÖ Excellent fact incorporation (mean: {avg_correctness:.3f})\")\n",
    "    elif avg_correctness > 0.6:\n",
    "        print(f\"‚úÖ Good fact incorporation (mean: {avg_correctness:.3f})\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Fact incorporation needs improvement (mean: {avg_correctness:.3f})\")\n",
    "    \n",
    "    avg_coherence = np.mean(coherence_scores)\n",
    "    if avg_coherence > 0.8:\n",
    "        print(f\"‚úÖ High text coherence (mean: {avg_coherence:.3f})\")\n",
    "    elif avg_coherence > 0.6:\n",
    "        print(f\"‚úÖ Acceptable text coherence (mean: {avg_coherence:.3f})\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Text coherence needs improvement (mean: {avg_coherence:.3f})\")\n",
    "    \n",
    "    avg_dissimilarity = np.mean(dissimilarity_scores)\n",
    "    if avg_dissimilarity > 0.3:\n",
    "        print(f\"‚úÖ Good dissimilarity from original (mean: {avg_dissimilarity:.3f})\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Synthetic articles too similar to originals (mean: {avg_dissimilarity:.3f})\")\n",
    "        \n",
    "else:\n",
    "    print(\"No results available for automatic evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11826e5",
   "metadata": {},
   "source": [
    "## Next Steps Based on Methodology\n",
    "\n",
    "### Current Status:\n",
    "- ‚úÖ **Step 1**: Data collection ready\n",
    "- ‚úÖ **Step 2**: COVID fact schema defined\n",
    "- ‚úÖ **Step 3**: Structured fact extraction implemented\n",
    "- ‚úÖ **Step 4**: Fact modification and synthetic generation working\n",
    "\n",
    "### Workflow Continuation:\n",
    "\n",
    "1. **Manual Evaluation** (Current)\n",
    "   - Share evaluation template with 3+ annotators\n",
    "   - Evaluate fact extraction and modification quality\n",
    "   - Calculate inter-annotator agreement\n",
    "\n",
    "2. **If Manual Evaluation Shows Good Results**:\n",
    "   - Scale to 100 news articles\n",
    "   - Scale to 100 tweets\n",
    "   - Repeat evaluation process\n",
    "\n",
    "3. **If Evaluation Shows Issues**:\n",
    "   - Refine fact characterization schema\n",
    "   - Adjust LLM prompts\n",
    "   - Re-run pipeline on sample data\n",
    "\n",
    "4. **Full Dataset Processing**:\n",
    "   - Process complete dataset\n",
    "   - Generate final synthetic dataset\n",
    "   - Prepare for classification experiments\n",
    "\n",
    "5. **Classification Phase**:\n",
    "   - Train ML/DL models on synthetic data\n",
    "   - Test on real-world fake news datasets\n",
    "   - Compare with models trained on real labeled data\n",
    "\n",
    "### Files Generated:\n",
    "- `data/synthetic/synthetic_data_pipeline_[timestamp].json` - Full results\n",
    "- `data/synthetic/synthetic_data_pipeline_[timestamp].csv` - CSV format\n",
    "- `results/evaluation_template_[timestamp].csv` - Manual annotation template"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
