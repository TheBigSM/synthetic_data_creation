{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d73d5d05",
   "metadata": {},
   "source": [
    "# Batch Processing for Synthetic Data Generation\n",
    "\n",
    "This notebook implements the recommended 3-phase approach for synthetic data generation.\n",
    "\n",
    "## üìã Recommended Procedure:\n",
    "\n",
    "### Phase 1: Initial Testing (10 articles + 10 tweets)\n",
    "- Test fact extraction and modification process\n",
    "- Verify system setup and API integration\n",
    "- Quick quality check before scaling\n",
    "\n",
    "### Phase 2: Manual Evaluation (100 articles + 100 tweets) \n",
    "- Generate larger sample for comprehensive evaluation\n",
    "- **Manual evaluation** by 3+ annotators using evaluation notebooks\n",
    "- **Automatic evaluation** with correctness, coherence, dissimilarity metrics\n",
    "- Check inter-annotator agreement\n",
    "- **Decision point**: Proceed only if quality is satisfactory\n",
    "\n",
    "### Phase 3: Full Dataset Processing (700 articles + 700 tweets)\n",
    "- Process complete dataset after validation\n",
    "- Continue to classification training\n",
    "- Final automatic evaluation of all results\n",
    "\n",
    "## üéØ Fact Characterization Strategy:\n",
    "Based on your dataset analysis, vaccination news articles focus on:\n",
    "- **Vaccine types** (Pfizer, Moderna, AstraZeneca, etc.)\n",
    "- **Effects and side effects**\n",
    "- **Death and injury statistics**  \n",
    "- **Regulatory and policy information**\n",
    "\n",
    "The fact schemas are designed to capture these patterns consistently.\n",
    "\n",
    "## ‚è±Ô∏è Processing Estimates:\n",
    "- **Phase 1**: ~34 minutes (10+10 items)\n",
    "- **Phase 2**: ~5.6 hours (100+100 items) \n",
    "- **Phase 3**: ~38.8 hours (700+700 items)\n",
    "- **Total cost**: ~$5.60 for all phases\n",
    "\n",
    "**Current Phase**: Start with Phase 1 below ‚¨áÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c59bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "# Import custom modules\n",
    "from src.data_generation.llm_client import create_llm_client, SyntheticDataResult\n",
    "from src.utils.data_utils import load_config, load_raw_data, save_processed_data\n",
    "from src.data_generation.fact_schemas import get_fact_schema\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Current time: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8189237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "config = load_config()\n",
    "llm_provider = \"together\"  # Using Together.ai\n",
    "\n",
    "print(\"üîß CONFIGURATION\")\n",
    "print(\"=\"*40)\n",
    "print(f\"LLM Provider: {llm_provider}\")\n",
    "print(f\"Model: {config['llm'][llm_provider]['model']}\")\n",
    "print(f\"Temperature: {config['llm'][llm_provider]['temperature']}\")\n",
    "print(f\"Max Tokens: {config['llm'][llm_provider]['max_tokens']}\")\n",
    "\n",
    "# Rate limiting settings\n",
    "REQUEST_DELAY = 100  # 100 seconds between requests (conservative)\n",
    "BATCH_SIZE = 10      # Save progress every 10 items\n",
    "MAX_RETRIES = 3      # Retry failed requests\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è RATE LIMITING\")\n",
    "print(f\"Delay between requests: {REQUEST_DELAY}s\")\n",
    "print(f\"Batch size for saves: {BATCH_SIZE}\")\n",
    "print(f\"Max retries: {MAX_RETRIES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60480346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM client\n",
    "try:\n",
    "    llm_client = create_llm_client(\n",
    "        provider=llm_provider,\n",
    "        model_name=config['llm'][llm_provider]['model'],\n",
    "        temperature=config['llm'][llm_provider]['temperature'],\n",
    "        max_tokens=config['llm'][llm_provider]['max_tokens']\n",
    "    )\n",
    "    print(f\"‚úÖ Successfully initialized {llm_provider} client\")\n",
    "    print(f\"Model: {config['llm'][llm_provider]['model']}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error initializing LLM client: {e}\")\n",
    "    print(\"Please check your TOGETHER_API_KEY in .env file.\")\n",
    "    llm_client = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab392ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your datasets\n",
    "print(\"üìä LOADING DATASETS\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "try:\n",
    "    # Load news articles\n",
    "    news_df = pd.read_csv(\"../../data/raw/vaccination_all_news.csv\")\n",
    "    print(f\"‚úÖ Loaded {len(news_df):,} news articles (will process 700)\")\n",
    "    print(f\"Columns: {list(news_df.columns)}\")\n",
    "    \n",
    "    # Take first 700 articles\n",
    "    news_df = news_df.head(700)\n",
    "    print(f\"üìä Using {len(news_df)} articles for processing\")\n",
    "    \n",
    "    # Display first few rows to understand structure\n",
    "    print(\"\\nFirst few rows of news data:\")\n",
    "    print(news_df.head(2))\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå vaccination_all_news.csv not found in data/raw/\")\n",
    "    news_df = None\n",
    "\n",
    "try:\n",
    "    # Load tweets\n",
    "    tweets_df = pd.read_csv(\"../../data/raw/vaccination_all_tweets.csv\")\n",
    "    print(f\"\\n‚úÖ Loaded {len(tweets_df):,} tweets (will process 700)\")\n",
    "    print(f\"Columns: {list(tweets_df.columns)}\")\n",
    "    \n",
    "    # Take first 700 tweets\n",
    "    tweets_df = tweets_df.head(700)\n",
    "    print(f\"üìä Using {len(tweets_df)} tweets for processing\")\n",
    "    \n",
    "    # Display first few rows\n",
    "    print(\"\\nFirst few rows of tweet data:\")\n",
    "    print(tweets_df.head(2))\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå vaccination_all_tweets.csv not found in data/raw/\")\n",
    "    tweets_df = None\n",
    "\n",
    "# Calculate processing estimates for 700 items each\n",
    "if news_df is not None:\n",
    "    news_time_hours = (len(news_df) * REQUEST_DELAY) / 3600\n",
    "    print(f\"\\n‚è±Ô∏è Processing time for 700 news articles: {news_time_hours:.1f} hours ({news_time_hours/24:.1f} days)\")\n",
    "\n",
    "if tweets_df is not None:\n",
    "    tweets_time_hours = (len(tweets_df) * REQUEST_DELAY) / 3600\n",
    "    print(f\"‚è±Ô∏è Processing time for 700 tweets: {tweets_time_hours:.1f} hours ({tweets_time_hours/24:.1f} days)\")\n",
    "\n",
    "if news_df is not None and tweets_df is not None:\n",
    "    total_time_hours = news_time_hours + tweets_time_hours\n",
    "    total_cost = (700 + 700) * 0.004  # Estimated cost per item\n",
    "    print(f\"\\nüí∞ TOTAL ESTIMATES:\")\n",
    "    print(f\"   Total processing time: {total_time_hours:.1f} hours ({total_time_hours/24:.1f} days)\")\n",
    "    print(f\"   Estimated total cost: ${total_cost:.2f}\")\n",
    "    print(f\"   Items to process: {700 + 700:,} total (700 articles + 700 tweets)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa4ba76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch processing function with error handling and progress tracking\n",
    "def process_batch_robust(data_items: List[str], \n",
    "                        content_type: str,\n",
    "                        start_index: int = 0,\n",
    "                        max_items: Optional[int] = None,\n",
    "                        resume_file: Optional[str] = None) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Robust batch processing with error handling and progress saving\n",
    "    \n",
    "    Args:\n",
    "        data_items: List of content strings to process\n",
    "        content_type: \"news\" or \"tweets\"\n",
    "        start_index: Index to start processing from (for resuming)\n",
    "        max_items: Maximum number of items to process (None for all)\n",
    "        resume_file: File to resume from if it exists\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load fact schema\n",
    "    fact_schema = get_fact_schema(content_type)\n",
    "    \n",
    "    # Resume from existing file if specified\n",
    "    results = []\n",
    "    if resume_file and os.path.exists(resume_file):\n",
    "        with open(resume_file, 'r') as f:\n",
    "            results = json.load(f)\n",
    "        print(f\"üìÅ Resumed from {resume_file} with {len(results)} existing results\")\n",
    "        start_index = len(results)\n",
    "    \n",
    "    # Determine processing range\n",
    "    end_index = len(data_items) if max_items is None else min(start_index + max_items, len(data_items))\n",
    "    items_to_process = data_items[start_index:end_index]\n",
    "    \n",
    "    print(f\"\\nüöÄ STARTING BATCH PROCESSING\")\n",
    "    print(f\"Content type: {content_type}\")\n",
    "    print(f\"Processing items {start_index} to {end_index-1} ({len(items_to_process)} items)\")\n",
    "    print(f\"Estimated time: {(len(items_to_process) * REQUEST_DELAY) / 3600:.1f} hours\")\n",
    "    \n",
    "    # Create results directory\n",
    "    os.makedirs(\"../../results\", exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    progress_file = f\"../../results/{content_type}_batch_progress_{timestamp}.json\"\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    for i, content in enumerate(tqdm(items_to_process, desc=f\"Processing {content_type}\")):\n",
    "        actual_index = start_index + i\n",
    "        retry_count = 0\n",
    "        success = False\n",
    "        \n",
    "        while retry_count < MAX_RETRIES and not success:\n",
    "            try:\n",
    "                print(f\"\\n--- Processing item {actual_index + 1}/{len(data_items)} (attempt {retry_count + 1}) ---\")\n",
    "                \n",
    "                # Extract facts with 3-fact limit\n",
    "                extracted_facts = llm_client.extract_structured_facts(content, fact_schema, max_facts=3)\n",
    "                \n",
    "                if extracted_facts.extracted_facts:\n",
    "                    # Modify facts\n",
    "                    modified_facts = llm_client.modify_facts(extracted_facts.extracted_facts)\n",
    "                    \n",
    "                    # Generate synthetic content\n",
    "                    synthetic_content = llm_client.generate_synthetic_article(content, modified_facts)\n",
    "                    \n",
    "                    result = {\n",
    "                        'generation_info': {\n",
    "                            'timestamp': datetime.now().isoformat(),\n",
    "                            'content_type': content_type,\n",
    "                            'model': config['llm'][llm_provider]['model'],\n",
    "                            'provider': llm_provider,\n",
    "                            'max_facts_limit': 3,\n",
    "                            'index': actual_index\n",
    "                        },\n",
    "                        'original_content': content,\n",
    "                        'extracted_facts': extracted_facts.extracted_facts,\n",
    "                        'modified_facts': modified_facts,\n",
    "                        'synthetic_content': synthetic_content,\n",
    "                        'fact_count': {\n",
    "                            'extracted': len(extracted_facts.extracted_facts),\n",
    "                            'modified': len(modified_facts)\n",
    "                        }\n",
    "                    }\n",
    "                    \n",
    "                    results.append(result)\n",
    "                    success = True\n",
    "                    print(f\"‚úÖ Successfully processed item {actual_index + 1}\")\n",
    "                    print(f\"   Extracted: {len(extracted_facts.extracted_facts)} facts\")\n",
    "                    print(f\"   Modified: {len(modified_facts)} facts\")\n",
    "                    \n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è No facts extracted from item {actual_index + 1}\")\n",
    "                    success = True  # Don't retry if no facts found\n",
    "                    \n",
    "            except Exception as e:\n",
    "                retry_count += 1\n",
    "                print(f\"‚ùå Error processing item {actual_index + 1} (attempt {retry_count}): {e}\")\n",
    "                \n",
    "                if retry_count < MAX_RETRIES:\n",
    "                    print(f\"   Retrying in {REQUEST_DELAY}s...\")\n",
    "                    time.sleep(REQUEST_DELAY)\n",
    "                else:\n",
    "                    print(f\"   Giving up after {MAX_RETRIES} attempts\")\n",
    "        \n",
    "        # Save progress every BATCH_SIZE items\n",
    "        if (i + 1) % BATCH_SIZE == 0 or i == len(items_to_process) - 1:\n",
    "            with open(progress_file, 'w') as f:\n",
    "                json.dump(results, f, indent=2)\n",
    "            \n",
    "            elapsed = datetime.now() - start_time\n",
    "            items_processed = len(results)\n",
    "            if items_processed > 0:\n",
    "                avg_time_per_item = elapsed.total_seconds() / items_processed\n",
    "                remaining_items = len(items_to_process) - (i + 1)\n",
    "                eta = datetime.now() + timedelta(seconds=remaining_items * avg_time_per_item)\n",
    "                \n",
    "                print(f\"\\nüíæ Progress saved: {items_processed}/{len(items_to_process)} items\")\n",
    "                print(f\"   Elapsed time: {elapsed}\")\n",
    "                print(f\"   ETA: {eta.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "        # Rate limiting - wait between requests\n",
    "        if i < len(items_to_process) - 1:\n",
    "            print(f\"‚è≥ Waiting {REQUEST_DELAY}s for rate limiting...\")\n",
    "            time.sleep(REQUEST_DELAY)\n",
    "    \n",
    "    # Final save\n",
    "    final_file = f\"../../results/{content_type}_batch_final_{timestamp}.json\"\n",
    "    with open(final_file, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    total_time = datetime.now() - start_time\n",
    "    print(f\"\\nüéâ BATCH PROCESSING COMPLETED!\")\n",
    "    print(f\"Total items processed: {len(results)}/{len(items_to_process)}\")\n",
    "    print(f\"Total time: {total_time}\")\n",
    "    print(f\"Final results saved to: {final_file}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Batch processing function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557fa72a",
   "metadata": {},
   "source": [
    "## Processing Options\n",
    "\n",
    "Choose one of the options below based on your needs:\n",
    "\n",
    "### Option 1: Small Test Batch (Recommended for first run)\n",
    "Process 10-20 items to test your setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13334cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PHASE 1: Initial Testing - 10 News Articles\n",
    "if llm_client and news_df is not None:\n",
    "    print(\"üß™ PHASE 1: INITIAL TESTING - 10 NEWS ARTICLES\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"This is the first phase to test your setup and verify fact extraction quality.\")\n",
    "    \n",
    "    # Use the content column - adjust column name if different\n",
    "    content_column = 'content'  # Change this if your column name is different\n",
    "    \n",
    "    if content_column in news_df.columns:\n",
    "        phase1_articles = news_df[content_column].head(10).tolist()\n",
    "        \n",
    "        phase1_results = process_batch_robust(\n",
    "            data_items=phase1_articles,\n",
    "            content_type=\"news\",\n",
    "            max_items=10\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nüìä PHASE 1 RESULTS SUMMARY:\")\n",
    "        print(f\"Successfully processed: {len(phase1_results)}/10 articles\")\n",
    "        \n",
    "        if phase1_results:\n",
    "            total_facts = sum(r['fact_count']['extracted'] for r in phase1_results)\n",
    "            print(f\"Total facts extracted: {total_facts}\")\n",
    "            print(f\"Average facts per article: {total_facts/len(phase1_results):.1f}\")\n",
    "            \n",
    "            # Save Phase 1 results for evaluation\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            phase1_file = f\"../../results/phase1_news_{timestamp}.json\"\n",
    "            with open(phase1_file, 'w') as f:\n",
    "                json.dump(phase1_results, f, indent=2)\n",
    "            print(f\"üíæ Phase 1 results saved: {phase1_file}\")\n",
    "            \n",
    "        print(f\"\\n‚úÖ Phase 1 completed successfully!\")\n",
    "        print(f\"üìã NEXT STEPS:\")\n",
    "        print(f\"1. Review the generated examples in results/\")\n",
    "        print(f\"2. If quality looks good, proceed to Phase 1 tweets\")\n",
    "        print(f\"3. After both Phase 1 tests, move to Phase 2 (100 items each)\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"‚ùå Column '{content_column}' not found in news data\")\n",
    "        print(f\"Available columns: {list(news_df.columns)}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping Phase 1 - no LLM client or data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99c517d",
   "metadata": {},
   "source": [
    "### Option 2: Medium Batch \n",
    "Process 50-100 items for more substantial testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d567cc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Medium batch (uncomment to run)\n",
    "# WARNING: This will take ~1.5 hours to complete\n",
    "\n",
    "\"\"\"\n",
    "if llm_client and news_df is not None:\n",
    "    print(\"üìà RUNNING MEDIUM BATCH - 50 NEWS ARTICLES\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"‚ö†Ô∏è This will take approximately 1.5 hours to complete\")\n",
    "    \n",
    "    content_column = 'content'  # Adjust if needed\n",
    "    \n",
    "    if content_column in news_df.columns:\n",
    "        medium_articles = news_df[content_column].head(50).tolist()\n",
    "        \n",
    "        medium_results = process_batch_robust(\n",
    "            data_items=medium_articles,\n",
    "            content_type=\"news\",\n",
    "            max_items=50\n",
    "        )\n",
    "        \n",
    "        print(f\"\\\\nüìä MEDIUM BATCH RESULTS:\")\n",
    "        print(f\"Successfully processed: {len(medium_results)}/50 articles\")\n",
    "        print(f\"Remaining articles to process: {len(news_df) - 50}\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"Medium batch code available (commented out for safety)\")\n",
    "print(\"After successful test, you can process larger batches or the full 700 articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31916562",
   "metadata": {},
   "source": [
    "### Option 3: Full Dataset Processing\n",
    "Process all your data (use with caution - will take days!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6158d819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 3: Full dataset processing - 700 articles\n",
    "# WARNING: This will take ~19.4 hours to complete\n",
    "\n",
    "\"\"\"\n",
    "if llm_client and news_df is not None:\n",
    "    print(\"üè≠ FULL DATASET PROCESSING - ALL 700 NEWS ARTICLES\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"‚ö†Ô∏è This will process all {len(news_df):,} articles\")\n",
    "    print(f\"‚ö†Ô∏è Estimated time: {(len(news_df) * REQUEST_DELAY) / 3600:.1f} hours\")\n",
    "    print(f\"‚ö†Ô∏è Estimated cost: ${len(news_df) * 0.004:.2f}\")\n",
    "    print(\"‚ö†Ô∏è Make sure you have sufficient API credits!\")\n",
    "    \n",
    "    # Uncomment below to run full processing\n",
    "    # content_column = 'content'\n",
    "    # \n",
    "    # if content_column in news_df.columns:\n",
    "    #     all_articles = news_df[content_column].tolist()\n",
    "    #     \n",
    "    #     full_results = process_batch_robust(\n",
    "    #         data_items=all_articles,\n",
    "    #         content_type=\"news\"\n",
    "    #     )\n",
    "    #\n",
    "    #     print(f\"\\\\nüéâ COMPLETED: Processed {len(full_results)}/700 articles\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"Full dataset processing code available (commented out for safety)\")\n",
    "print(\"Processes all 700 articles - uncomment when ready for full run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1f2271",
   "metadata": {},
   "source": [
    "### Tweet Processing\n",
    "Similar batch processing for tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea60ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PHASE 1: Initial Testing - 10 Tweets\n",
    "if llm_client and tweets_df is not None:\n",
    "    print(\"üê¶ PHASE 1: INITIAL TESTING - 10 TWEETS\")\n",
    "    print(\"=\"*40)\n",
    "    print(\"Complete Phase 1 testing with tweets to verify tweet processing.\")\n",
    "    \n",
    "    # Find the text column in tweets (common names: 'text', 'content', 'tweet_text')\n",
    "    text_columns = ['text', 'content', 'tweet_text', 'full_text']\n",
    "    tweet_text_column = None\n",
    "    \n",
    "    for col in text_columns:\n",
    "        if col in tweets_df.columns:\n",
    "            tweet_text_column = col\n",
    "            break\n",
    "    \n",
    "    if tweet_text_column:\n",
    "        print(f\"Using column '{tweet_text_column}' for tweet text\")\n",
    "        \n",
    "        # Process first 10 tweets for Phase 1\n",
    "        phase1_tweets = tweets_df[tweet_text_column].head(10).tolist()\n",
    "        \n",
    "        phase1_tweet_results = process_batch_robust(\n",
    "            data_items=phase1_tweets,\n",
    "            content_type=\"tweets\",\n",
    "            max_items=10\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nüìä PHASE 1 TWEET RESULTS:\")\n",
    "        print(f\"Successfully processed: {len(phase1_tweet_results)}/10 tweets\")\n",
    "        \n",
    "        if phase1_tweet_results:\n",
    "            total_facts = sum(r['fact_count']['extracted'] for r in phase1_tweet_results)\n",
    "            print(f\"Total facts extracted: {total_facts}\")\n",
    "            print(f\"Average facts per tweet: {total_facts/len(phase1_tweet_results):.1f}\")\n",
    "            \n",
    "            # Save Phase 1 tweet results\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            phase1_tweet_file = f\"../../results/phase1_tweets_{timestamp}.json\"\n",
    "            with open(phase1_tweet_file, 'w') as f:\n",
    "                json.dump(phase1_tweet_results, f, indent=2)\n",
    "            print(f\"üíæ Phase 1 tweet results saved: {phase1_tweet_file}\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ PHASE 1 COMPLETE!\")\n",
    "        print(f\"üìã EVALUATION & NEXT STEPS:\")\n",
    "        print(f\"1. Review both news and tweet results in ../../results/\")\n",
    "        print(f\"2. Check fact extraction quality manually\")\n",
    "        print(f\"3. If satisfied, proceed to Phase 2 (100 articles + 100 tweets)\")\n",
    "        print(f\"4. Use evaluation notebooks for comprehensive assessment\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"‚ùå Could not find tweet text column\")\n",
    "        print(f\"Available columns: {list(tweets_df.columns)}\")\n",
    "        print(\"Please manually specify the correct column name\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping Phase 1 tweets - no data or LLM client available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d606343",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "After running your batch processing:\n",
    "\n",
    "1. **Check Results**: Results are saved in `../../results/` directory\n",
    "2. **Monitor Progress**: Each batch saves intermediate progress files\n",
    "3. **Resume Processing**: You can resume from progress files if interrupted\n",
    "4. **Scale Gradually**: Start small, then increase batch sizes\n",
    "5. **Monitor Costs**: Together.ai charges $1.12 per 1M tokens\n",
    "\n",
    "### Your Dataset Processing Estimates:\n",
    "- **700 news articles**: ~19.4 hours, ~$2.80 cost\n",
    "- **700 tweets**: ~19.4 hours, ~$2.80 cost\n",
    "- **Total**: ~38.8 hours (1.6 days), ~$5.60 total cost\n",
    "\n",
    "### Recommended Processing Strategy:\n",
    "1. **Start Small**: Run test batches (5-10 items) first\n",
    "2. **Verify Quality**: Check generated examples in `results/` \n",
    "3. **Scale Up**: Process 50-100 items to test stability\n",
    "4. **Full Processing**: Run all 700 articles, then all 700 tweets\n",
    "5. **Monitor Progress**: Save points every 10 items allow resume if interrupted\n",
    "\n",
    "### Processing Schedule Recommendation:\n",
    "- **Day 1**: Test batches + first 100 articles (~3 hours)\n",
    "- **Day 2**: Remaining 600 articles (~17 hours) \n",
    "- **Day 3**: All 700 tweets (~19 hours)\n",
    "\n",
    "This gives you a manageable 3-day processing schedule with plenty of checkpoints!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf12925",
   "metadata": {},
   "source": [
    "## Phase 2: Evaluation Testing (100 Items Each)\n",
    "\n",
    "After reviewing Phase 1 results and confirming the system works correctly, proceed to Phase 2 for evaluation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8970da07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PHASE 2: Evaluation Testing - 100 Articles + 100 Tweets\n",
    "print(\"\\nüî¨ PHASE 2: EVALUATION TESTING - 100 ITEMS EACH\")\n",
    "print(\"=\"*50)\n",
    "print(\"Complete Phase 2 only after reviewing Phase 1 results.\")\n",
    "print(\"This phase provides data for manual and automatic evaluation.\")\n",
    "\n",
    "# Phase 2 control - SET TO TRUE AFTER PHASE 1 REVIEW\n",
    "run_phase2 = False  # Change to True when ready for Phase 2\n",
    "\n",
    "if run_phase2 and llm_client:\n",
    "    print(\"\\nüì∞ Processing 100 News Articles...\")\n",
    "    if news_df is not None:\n",
    "        phase2_articles = news_df['content'].head(100).tolist()\n",
    "        \n",
    "        phase2_news_results = process_batch_robust(\n",
    "            data_items=phase2_articles,\n",
    "            content_type=\"news articles\",\n",
    "            max_items=100\n",
    "        )\n",
    "        \n",
    "        print(f\"News articles processed: {len(phase2_news_results)}/100\")\n",
    "        \n",
    "        # Calculate Phase 2 news statistics\n",
    "        if phase2_news_results:\n",
    "            total_facts = sum(r['fact_count']['extracted'] for r in phase2_news_results)\n",
    "            print(f\"Total facts extracted: {total_facts}\")\n",
    "            print(f\"Average facts per article: {total_facts/len(phase2_news_results):.1f}\")\n",
    "    \n",
    "    print(\"\\nüê¶ Processing 100 Tweets...\")\n",
    "    if tweets_df is not None and 'tweet_text_column' in locals():\n",
    "        phase2_tweets = tweets_df[tweet_text_column].head(100).tolist()\n",
    "        \n",
    "        phase2_tweet_results = process_batch_robust(\n",
    "            data_items=phase2_tweets,\n",
    "            content_type=\"tweets\",\n",
    "            max_items=100\n",
    "        )\n",
    "        \n",
    "        print(f\"Tweets processed: {len(phase2_tweet_results)}/100\")\n",
    "        \n",
    "        # Calculate Phase 2 tweet statistics\n",
    "        if phase2_tweet_results:\n",
    "            total_facts = sum(r['fact_count']['extracted'] for r in phase2_tweet_results)\n",
    "            print(f\"Total facts extracted: {total_facts}\")\n",
    "            print(f\"Average facts per tweet: {total_facts/len(phase2_tweet_results):.1f}\")\n",
    "    \n",
    "    # Save Phase 2 results\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    if 'phase2_news_results' in locals():\n",
    "        phase2_news_file = f\"../../results/phase2_news_{timestamp}.json\"\n",
    "        with open(phase2_news_file, 'w') as f:\n",
    "            json.dump(phase2_news_results, f, indent=2)\n",
    "        print(f\"üíæ Phase 2 news results saved: {phase2_news_file}\")\n",
    "    \n",
    "    if 'phase2_tweet_results' in locals():\n",
    "        phase2_tweet_file = f\"../../results/phase2_tweets_{timestamp}.json\"\n",
    "        with open(phase2_tweet_file, 'w') as f:\n",
    "            json.dump(phase2_tweet_results, f, indent=2)\n",
    "        print(f\"üíæ Phase 2 tweet results saved: {phase2_tweet_file}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ PHASE 2 COMPLETE!\")\n",
    "    print(f\"üìã EVALUATION REQUIRED BEFORE PHASE 3:\")\n",
    "    print(f\"1. Use ../evaluation/01_manual_evaluation.ipynb for manual annotation\")  \n",
    "    print(f\"2. Recruit 3+ evaluators for inter-annotator agreement\")\n",
    "    print(f\"3. Use ../evaluation/02_automatic_evaluation.ipynb for automatic metrics\")\n",
    "    print(f\"4. Check Cohen's Kappa score (Œ∫):\")\n",
    "    print(f\"   - Œ∫ > 0.60: Proceed to Phase 3\")\n",
    "    print(f\"   - Œ∫ 0.40-0.60: Improve guidelines, re-evaluate\")\n",
    "    print(f\"   - Œ∫ < 0.40: Major methodology changes needed\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Phase 2 disabled. Set run_phase2 = True after Phase 1 review\")\n",
    "    print(\"üìã Before enabling Phase 2:\")\n",
    "    print(\"1. Review Phase 1 results thoroughly\")\n",
    "    print(\"2. Verify fact extraction quality\")\n",
    "    print(\"3. Check error handling works correctly\")\n",
    "    print(\"4. Ensure you're ready for evaluation process\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38865c2c",
   "metadata": {},
   "source": [
    "## Phase 3: Full Dataset Processing (700 Items Each)\n",
    "\n",
    "Only proceed to Phase 3 after successful Phase 2 evaluation with satisfactory inter-annotator agreement (Œ∫ > 0.60)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaa8c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PHASE 3: Full Dataset Processing - 700 Articles + 700 Tweets\n",
    "print(\"\\nüöÄ PHASE 3: FULL DATASET PROCESSING - 700 ITEMS EACH\")\n",
    "print(\"=\"*55)\n",
    "print(\"Complete Phase 3 only after successful Phase 2 evaluation.\")\n",
    "print(\"This is the final production run for the complete dataset.\")\n",
    "\n",
    "# Phase 3 control - SET TO TRUE AFTER SUCCESSFUL PHASE 2 EVALUATION\n",
    "run_phase3 = False  # Change to True after Phase 2 evaluation success\n",
    "\n",
    "if run_phase3 and llm_client:\n",
    "    print(\"\\nüìä ESTIMATED PROCESSING TIME & COST:\")\n",
    "    print(f\"- Articles: ~70 minutes (700 √ó 6s average)\")\n",
    "    print(f\"- Tweets: ~35 minutes (700 √ó 3s average)\")  \n",
    "    print(f\"- Total time: ~105 minutes (~1.75 hours)\")\n",
    "    print(f\"- Estimated cost: ~$5.60 (1.4M items √ó $4/1M tokens)\")\n",
    "    print(f\"- Rate limit: 100 requests/second (should be sufficient)\")\n",
    "    \n",
    "    confirm = input(\"\\n‚ö†Ô∏è  FINAL CONFIRMATION: Process full dataset? (yes/no): \")\n",
    "    \n",
    "    if confirm.lower() == 'yes':\n",
    "        print(\"\\nüì∞ Processing 700 News Articles...\")\n",
    "        if news_df is not None:\n",
    "            # Take exactly 700 articles\n",
    "            phase3_articles = news_df['content'].head(700).tolist()\n",
    "            \n",
    "            phase3_news_results = process_batch_robust(\n",
    "                data_items=phase3_articles,\n",
    "                content_type=\"news articles\",\n",
    "                max_items=700\n",
    "            )\n",
    "            \n",
    "            print(f\"News articles processed: {len(phase3_news_results)}/700\")\n",
    "            \n",
    "            # Calculate comprehensive statistics\n",
    "            if phase3_news_results:\n",
    "                total_facts = sum(r['fact_count']['extracted'] for r in phase3_news_results)\n",
    "                total_modified = sum(r['fact_count']['modified'] for r in phase3_news_results)\n",
    "                success_rate = len(phase3_news_results) / 700 * 100\n",
    "                \n",
    "                print(f\"Success rate: {success_rate:.1f}%\")\n",
    "                print(f\"Total facts extracted: {total_facts}\")\n",
    "                print(f\"Total facts modified: {total_modified}\")\n",
    "                print(f\"Average facts per article: {total_facts/len(phase3_news_results):.1f}\")\n",
    "        \n",
    "        print(\"\\nüê¶ Processing 700 Tweets...\")\n",
    "        if tweets_df is not None and 'tweet_text_column' in locals():\n",
    "            # Take exactly 700 tweets\n",
    "            phase3_tweets = tweets_df[tweet_text_column].head(700).tolist()\n",
    "            \n",
    "            phase3_tweet_results = process_batch_robust(\n",
    "                data_items=phase3_tweets,\n",
    "                content_type=\"tweets\",\n",
    "                max_items=700\n",
    "            )\n",
    "            \n",
    "            print(f\"Tweets processed: {len(phase3_tweet_results)}/700\")\n",
    "            \n",
    "            # Calculate comprehensive statistics\n",
    "            if phase3_tweet_results:\n",
    "                total_facts = sum(r['fact_count']['extracted'] for r in phase3_tweet_results)\n",
    "                total_modified = sum(r['fact_count']['modified'] for r in phase3_tweet_results)\n",
    "                success_rate = len(phase3_tweet_results) / 700 * 100\n",
    "                \n",
    "                print(f\"Success rate: {success_rate:.1f}%\")\n",
    "                print(f\"Total facts extracted: {total_facts}\")\n",
    "                print(f\"Total facts modified: {total_modified}\")\n",
    "                print(f\"Average facts per tweet: {total_facts/len(phase3_tweet_results):.1f}\")\n",
    "        \n",
    "        # Save Phase 3 results with detailed metadata\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        if 'phase3_news_results' in locals():\n",
    "            phase3_news_file = f\"../../results/FINAL_news_articles_{timestamp}.json\"\n",
    "            \n",
    "            # Add metadata to results\n",
    "            final_news_data = {\n",
    "                \"metadata\": {\n",
    "                    \"processing_date\": timestamp,\n",
    "                    \"phase\": \"Phase 3 - Full Dataset\",\n",
    "                    \"content_type\": \"news_articles\", \n",
    "                    \"total_items\": 700,\n",
    "                    \"successful_items\": len(phase3_news_results),\n",
    "                    \"success_rate\": len(phase3_news_results) / 700 * 100\n",
    "                },\n",
    "                \"results\": phase3_news_results\n",
    "            }\n",
    "            \n",
    "            with open(phase3_news_file, 'w') as f:\n",
    "                json.dump(final_news_data, f, indent=2)\n",
    "            print(f\"üíæ Phase 3 news results saved: {phase3_news_file}\")\n",
    "        \n",
    "        if 'phase3_tweet_results' in locals():\n",
    "            phase3_tweet_file = f\"../../results/FINAL_tweets_{timestamp}.json\"\n",
    "            \n",
    "            # Add metadata to results\n",
    "            final_tweet_data = {\n",
    "                \"metadata\": {\n",
    "                    \"processing_date\": timestamp,\n",
    "                    \"phase\": \"Phase 3 - Full Dataset\", \n",
    "                    \"content_type\": \"tweets\",\n",
    "                    \"total_items\": 700,\n",
    "                    \"successful_items\": len(phase3_tweet_results),\n",
    "                    \"success_rate\": len(phase3_tweet_results) / 700 * 100\n",
    "                },\n",
    "                \"results\": phase3_tweet_results\n",
    "            }\n",
    "            \n",
    "            with open(phase3_tweet_file, 'w') as f:\n",
    "                json.dump(final_tweet_data, f, indent=2)\n",
    "            print(f\"üíæ Phase 3 tweet results saved: {phase3_tweet_file}\")\n",
    "        \n",
    "        print(f\"\\nüéâ PHASE 3 COMPLETE - FULL DATASET PROCESSED!\")\n",
    "        print(f\"üìã FINAL SUMMARY:\")\n",
    "        print(f\"- Total items processed: {len(locals().get('phase3_news_results', [])) + len(locals().get('phase3_tweet_results', []))} / 1400\")\n",
    "        print(f\"- Results saved with FINAL_ prefix for easy identification\")\n",
    "        print(f\"- Ready for final analysis and classification tasks\")\n",
    "        print(f\"- All phases completed successfully! üöÄ\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå Phase 3 cancelled by user\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Phase 3 disabled. Set run_phase3 = True after Phase 2 evaluation\")\n",
    "    print(\"üìã Before enabling Phase 3:\")\n",
    "    print(\"1. Complete Phase 2 evaluation successfully\")\n",
    "    print(\"2. Achieve inter-annotator agreement Œ∫ > 0.60\")\n",
    "    print(\"3. Verify automatic evaluation metrics are satisfactory\")\n",
    "    print(\"4. Ensure sufficient resources for full processing\")\n",
    "    print(\"5. Have ~2 hours available for uninterrupted processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab65af3",
   "metadata": {},
   "source": [
    "## Final Summary - Complete 3-Phase Processing Approach\n",
    "\n",
    "This notebook implements a structured 3-phase approach for synthetic data generation:\n",
    "\n",
    "**Phase 1** (10+10 items): Initial testing and system validation  \n",
    "**Phase 2** (100+100 items): Evaluation and quality assessment  \n",
    "**Phase 3** (700+700 items): Full dataset processing  \n",
    "\n",
    "### Evaluation Requirements\n",
    "- **Manual Evaluation**: Use `../evaluation/01_manual_evaluation.ipynb`\n",
    "  - 3+ annotators required for inter-annotator agreement\n",
    "  - Cohen's Kappa (Œ∫) threshold: Œ∫ > 0.60 to proceed\n",
    "- **Automatic Evaluation**: Use `../evaluation/02_automatic_evaluation.ipynb`  \n",
    "  - Correctness, coherence, and dissimilarity metrics\n",
    "  - Quantitative assessment of generated content quality\n",
    "\n",
    "### Next Steps After Processing\n",
    "1. Complete all 3 phases with proper evaluation checkpoints\n",
    "2. Use evaluation results for classification model training\n",
    "3. Document findings and methodology for research publication"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
