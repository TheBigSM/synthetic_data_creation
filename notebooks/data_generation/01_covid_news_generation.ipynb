{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0e31003",
   "metadata": {},
   "source": [
    "# COVID News Synthetic Data Generation - Structured Methodology\n",
    "\n",
    "This notebook implements the 4-step methodology for synthetic data generation:\n",
    "\n",
    "1. **Data Collection** (already done) - Real COVID news articles collected\n",
    "2. **Fact Characterization** - Define fact types with name, description, and examples\n",
    "3. **Fact Extraction** - Extract structured facts using LLM with predefined schema\n",
    "4. **Fact Manipulation** - Modify facts and generate synthetic fake news articles\n",
    "\n",
    "## Output Format\n",
    "The pipeline generates JSON output with:\n",
    "- `original_article` - Original news article\n",
    "- `extracted_facts` - Structured facts with name, description, specific_data\n",
    "- `modified_facts` - Modified facts before replacement\n",
    "- `modified_article` - Final synthetic article\n",
    "\n",
    "This follows the methodology for creating plausible but false COVID-related news content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ef2507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Import custom modules\n",
    "from src.data_generation.llm_client import create_llm_client, SyntheticDataResult\n",
    "from src.utils.data_utils import load_config, load_raw_data, save_processed_data\n",
    "from src.utils.text_preprocessing import TextPreprocessor\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006d63ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = load_config()\n",
    "print(\"Configuration loaded:\")\n",
    "print(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb1dceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables (API keys)\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Check if API keys are available\n",
    "openai_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "\n",
    "print(f\"OpenAI API Key available: {openai_key is not None}\")\n",
    "print(f\"Anthropic API Key available: {anthropic_key is not None}\")\n",
    "\n",
    "if not openai_key and not anthropic_key:\n",
    "    print(\"\\nWarning: No API keys found. Please set up your .env file with API keys.\")\n",
    "    print(\"Copy .env.example to .env and fill in your API keys.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8e4acb",
   "metadata": {},
   "source": [
    "## Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea98a145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw COVID news data\n",
    "# Note: You'll need to add your COVID news dataset to data/raw/covid_news.csv\n",
    "# Expected columns: 'title', 'content', 'date', 'source', etc.\n",
    "\n",
    "try:\n",
    "    news_df = load_raw_data(\"news\")\n",
    "    print(f\"Loaded {len(news_df)} news articles\")\n",
    "    \n",
    "    if len(news_df) > 0:\n",
    "        print(\"\\nDataset info:\")\n",
    "        print(news_df.info())\n",
    "        print(\"\\nFirst few rows:\")\n",
    "        print(news_df.head())\n",
    "    else:\n",
    "        print(\"No data found. Please add your COVID news dataset to data/raw/covid_news.csv\")\n",
    "        \n",
    "        # Create sample data for demonstration\n",
    "        sample_articles = [\n",
    "            {\n",
    "                'title': 'New COVID-19 Variant Detected in Major City',\n",
    "                'content': 'Health officials in New York City announced today that a new variant of COVID-19 has been detected in 150 patients. The variant, designated B.1.1.7, appears to be 70% more transmissible than previous strains. Local hospitals have reported a 25% increase in admissions over the past week.',\n",
    "                'source': 'Health News Daily',\n",
    "                'date': '2024-01-15'\n",
    "            },\n",
    "            {\n",
    "                'title': 'Vaccine Effectiveness Study Results Released',\n",
    "                'content': 'A comprehensive study involving 50,000 participants shows that the latest COVID-19 vaccines maintain 95% effectiveness against severe illness. The research, conducted over 6 months, found that booster shots increase protection to 98% within two weeks of administration.',\n",
    "                'source': 'Medical Research Journal',\n",
    "                'date': '2024-01-10'\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        news_df = pd.DataFrame(sample_articles)\n",
    "        print(f\"\\nCreated sample dataset with {len(news_df)} articles for demonstration.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    news_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36264563",
   "metadata": {},
   "source": [
    "## LLM Client Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cd2de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM client\n",
    "llm_provider = \"openai\"  # Change to \"anthropic\" if you prefer Claude\n",
    "\n",
    "try:\n",
    "    llm_client = create_llm_client(\n",
    "        provider=llm_provider,\n",
    "        model_name=config['llm'][llm_provider]['model'],\n",
    "        temperature=config['llm'][llm_provider]['temperature'],\n",
    "        max_tokens=config['llm'][llm_provider]['max_tokens']\n",
    "    )\n",
    "    print(f\"Successfully initialized {llm_provider} client\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing LLM client: {e}\")\n",
    "    print(\"Please check your API keys and configuration.\")\n",
    "    llm_client = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fca4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Fact Characterization - COVID-specific fact schema\n",
    "# Following methodology: define facts with name, description, and common examples\n",
    "\n",
    "from src.data_generation.fact_schemas import get_fact_schema, display_fact_schema, COVID_FACT_SCHEMA\n",
    "\n",
    "# Load COVID-specific fact characterization schema\n",
    "covid_fact_schema = get_fact_schema(\"news\")\n",
    "\n",
    "print(\"STEP 2: FACT CHARACTERIZATION\")\n",
    "print(\"=\"*40)\n",
    "display_fact_schema(covid_fact_schema)\n",
    "\n",
    "print(f\"\\nFact schema loaded with {len(covid_fact_schema)} fact types\")\n",
    "print(\"This schema will guide structured fact extraction in Step 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d100a103",
   "metadata": {},
   "source": [
    "## Fact Extraction and Synthetic Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187f8845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 & 4: Structured fact extraction and manipulation\n",
    "def process_article_structured(article_text: str, llm_client, fact_schema: list) -> dict:\n",
    "    \"\"\"\n",
    "    Process article using structured 4-step methodology:\n",
    "    Step 3: Extract structured facts\n",
    "    Step 4: Modify facts and generate synthetic article\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"  Step 3: Extracting structured facts...\")\n",
    "        # Extract structured facts using schema\n",
    "        fact_result = llm_client.extract_structured_facts(article_text, fact_schema)\n",
    "        \n",
    "        if not fact_result.extracted_facts:\n",
    "            print(f\"  ⚠️  No facts extracted from article\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"  ✓ Extracted {len(fact_result.extracted_facts)} structured facts\")\n",
    "        \n",
    "        print(f\"  Step 4: Modifying facts...\")\n",
    "        # Modify facts to create false information\n",
    "        modified_facts = llm_client.modify_facts(fact_result.extracted_facts)\n",
    "        \n",
    "        print(f\"  ✓ Modified {len(modified_facts)} facts\")\n",
    "        \n",
    "        print(f\"  Step 4: Generating synthetic article...\")\n",
    "        # Generate synthetic article with modified facts\n",
    "        synthetic_article = llm_client.generate_synthetic_article(article_text, modified_facts)\n",
    "        \n",
    "        print(f\"  ✓ Generated synthetic article\")\n",
    "        \n",
    "        # Return in required JSON format\n",
    "        return {\n",
    "            'original_article': article_text,\n",
    "            'extracted_facts': fact_result.extracted_facts,\n",
    "            'modified_facts': modified_facts,\n",
    "            'modified_article': synthetic_article,\n",
    "            'metadata': {\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'model': llm_client.model_name,\n",
    "                'fact_schema_size': len(fact_schema)\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Error processing article: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"Structured processing function defined (following 4-step methodology).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fbc327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process articles using structured methodology (Steps 3 & 4)\n",
    "if llm_client is not None and len(news_df) > 0:\n",
    "    \n",
    "    print(\"PROCESSING ARTICLES WITH STRUCTURED METHODOLOGY\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    synthetic_results = []\n",
    "    \n",
    "    # Start with 10 articles as per methodology (then 100, then full dataset)\n",
    "    sample_size = min(10, len(news_df))\n",
    "    \n",
    "    print(f\"Processing {sample_size} articles using 4-step methodology...\")\n",
    "    print(\"Following workflow: 10 articles → manual evaluation → 100 articles → full dataset\")\n",
    "    \n",
    "    for idx, row in tqdm(news_df.head(sample_size).iterrows(), total=sample_size):\n",
    "        # Use content column, or combine title and content\n",
    "        if 'content' in row and pd.notna(row['content']):\n",
    "            article_text = row['content']\n",
    "        else:\n",
    "            article_text = row['title'] if 'title' in row else str(row.iloc[0])\n",
    "        \n",
    "        print(f\"\\n📄 Processing article {idx + 1}/{sample_size}...\")\n",
    "        \n",
    "        # Use structured processing function\n",
    "        result = process_article_structured(article_text, llm_client, covid_fact_schema)\n",
    "        \n",
    "        if result:\n",
    "            synthetic_results.append(result)\n",
    "            print(f\"  ✅ Successfully generated synthetic version\")\n",
    "        else:\n",
    "            print(f\"  ❌ Failed to generate synthetic version\")\n",
    "    \n",
    "    print(f\"\\n🎉 COMPLETED: Generated {len(synthetic_results)} synthetic articles\")\n",
    "    print(f\"Success rate: {len(synthetic_results)/sample_size*100:.1f}%\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️  Skipping synthetic data generation due to missing LLM client or data.\")\n",
    "    synthetic_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2949dd08",
   "metadata": {},
   "source": [
    "## Results Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d60552d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display structured results in required JSON format\n",
    "if synthetic_results:\n",
    "    print(\"STRUCTURED SYNTHETIC DATA RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"Format: original_article, extracted_facts, modified_facts, modified_article\")\n",
    "    \n",
    "    for i, result in enumerate(synthetic_results[:2]):  # Show first 2 in detail\n",
    "        print(f\"\\n📰 ARTICLE {i + 1}\")\n",
    "        print(\"-\"*30)\n",
    "        \n",
    "        print(\"\\n\udd39 ORIGINAL ARTICLE:\")\n",
    "        print(f\"  {result['original_article'][:200]}{'...' if len(result['original_article']) > 200 else ''}\")\n",
    "        \n",
    "        print(\"\\n🔹 EXTRACTED FACTS (Step 3):\")\n",
    "        for j, fact in enumerate(result['extracted_facts'], 1):\n",
    "            print(f\"  {j}. {fact.get('name_of_fact', 'Unknown')}\")\n",
    "            print(f\"     Description: {fact.get('description_of_fact', 'N/A')}\")\n",
    "            print(f\"     Specific Data: {fact.get('specific_data', 'N/A')}\")\n",
    "        \n",
    "        print(\"\\n\udd39 MODIFIED FACTS (Step 4 - before replacement):\")\n",
    "        for j, fact in enumerate(result['modified_facts'], 1):\n",
    "            print(f\"  {j}. {fact.get('name_of_fact', 'Unknown')}\")\n",
    "            print(f\"     Modified Data: {fact.get('specific_data', 'N/A')}\")\n",
    "        \n",
    "        print(\"\\n\udd39 MODIFIED ARTICLE (Step 4 - final output):\")\n",
    "        print(f\"  {result['modified_article'][:200]}{'...' if len(result['modified_article']) > 200 else ''}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "    \n",
    "    if len(synthetic_results) > 2:\n",
    "        print(f\"\\n... and {len(synthetic_results) - 2} more results\")\n",
    "        \n",
    "    print(f\"\\n📊 SUMMARY:\")\n",
    "    print(f\"  - Total articles processed: {len(synthetic_results)}\")\n",
    "    total_facts = sum(len(r['extracted_facts']) for r in synthetic_results)\n",
    "    print(f\"  - Total facts extracted: {total_facts}\")\n",
    "    print(f\"  - Average facts per article: {total_facts/len(synthetic_results):.1f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No synthetic results to display.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18069e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary statistics\n",
    "if synthetic_results:\n",
    "    # Create DataFrame for analysis\n",
    "    analysis_data = []\n",
    "    \n",
    "    for result in synthetic_results:\n",
    "        analysis_data.append({\n",
    "            'original_length': len(result.original_text),\n",
    "            'synthetic_length': len(result.synthetic_text),\n",
    "            'original_word_count': len(result.original_text.split()),\n",
    "            'synthetic_word_count': len(result.synthetic_text.split()),\n",
    "            'facts_extracted': len(result.original_facts),\n",
    "            'facts_modified': len(result.modified_facts)\n",
    "        })\n",
    "    \n",
    "    analysis_df = pd.DataFrame(analysis_data)\n",
    "    \n",
    "    # Display summary statistics\n",
    "    print(\"GENERATION STATISTICS\")\n",
    "    print(\"=\" * 30)\n",
    "    print(analysis_df.describe())\n",
    "    \n",
    "    # Visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    fig.suptitle('Synthetic Data Generation Analysis', fontsize=16)\n",
    "    \n",
    "    # Text length comparison\n",
    "    axes[0, 0].scatter(analysis_df['original_length'], analysis_df['synthetic_length'])\n",
    "    axes[0, 0].plot([0, analysis_df['original_length'].max()], [0, analysis_df['original_length'].max()], 'r--', alpha=0.5)\n",
    "    axes[0, 0].set_xlabel('Original Text Length')\n",
    "    axes[0, 0].set_ylabel('Synthetic Text Length')\n",
    "    axes[0, 0].set_title('Text Length Comparison')\n",
    "    \n",
    "    # Word count comparison\n",
    "    axes[0, 1].scatter(analysis_df['original_word_count'], analysis_df['synthetic_word_count'])\n",
    "    axes[0, 1].plot([0, analysis_df['original_word_count'].max()], [0, analysis_df['original_word_count'].max()], 'r--', alpha=0.5)\n",
    "    axes[0, 1].set_xlabel('Original Word Count')\n",
    "    axes[0, 1].set_ylabel('Synthetic Word Count')\n",
    "    axes[0, 1].set_title('Word Count Comparison')\n",
    "    \n",
    "    # Facts extracted distribution\n",
    "    axes[1, 0].hist(analysis_df['facts_extracted'], bins=5, alpha=0.7, edgecolor='black')\n",
    "    axes[1, 0].set_xlabel('Number of Facts Extracted')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].set_title('Facts Extracted Distribution')\n",
    "    \n",
    "    # Facts modified distribution\n",
    "    axes[1, 1].hist(analysis_df['facts_modified'], bins=5, alpha=0.7, edgecolor='black')\n",
    "    axes[1, 1].set_xlabel('Number of Facts Modified')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    axes[1, 1].set_title('Facts Modified Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"No data available for analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b31e5c",
   "metadata": {},
   "source": [
    "## Save Generated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d99985f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save synthetic data to files\n",
    "if synthetic_results:\n",
    "    # Create dataset with original and synthetic pairs\n",
    "    synthetic_dataset = []\n",
    "    \n",
    "    for result in synthetic_results:\n",
    "        # Add original (real) article\n",
    "        synthetic_dataset.append({\n",
    "            'text': result.original_text,\n",
    "            'label': 'real',\n",
    "            'type': 'original',\n",
    "            'facts': '|'.join(result.original_facts),\n",
    "            'generation_metadata': None\n",
    "        })\n",
    "        \n",
    "        # Add synthetic (fake) article\n",
    "        synthetic_dataset.append({\n",
    "            'text': result.synthetic_text,\n",
    "            'label': 'fake',\n",
    "            'type': 'synthetic',\n",
    "            'facts': '|'.join(result.modified_facts),\n",
    "            'generation_metadata': json.dumps(result.generation_metadata)\n",
    "        })\n",
    "    \n",
    "    # Convert to DataFrame and save\n",
    "    synthetic_df = pd.DataFrame(synthetic_dataset)\n",
    "    \n",
    "    # Save to processed data folder\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"synthetic_covid_news_{timestamp}.csv\"\n",
    "    \n",
    "    save_processed_data(synthetic_df, filename, \"synthetic\")\n",
    "    \n",
    "    print(f\"Saved {len(synthetic_df)} records to data/synthetic/{filename}\")\n",
    "    print(f\"\\nDataset breakdown:\")\n",
    "    print(synthetic_df['label'].value_counts())\n",
    "    \n",
    "    # Also save detailed results as JSON\n",
    "    detailed_results = []\n",
    "    for result in synthetic_results:\n",
    "        detailed_results.append({\n",
    "            'original_text': result.original_text,\n",
    "            'original_facts': result.original_facts,\n",
    "            'modified_facts': result.modified_facts,\n",
    "            'synthetic_text': result.synthetic_text,\n",
    "            'metadata': result.generation_metadata\n",
    "        })\n",
    "    \n",
    "    json_filename = f\"detailed_generation_results_{timestamp}.json\"\n",
    "    json_path = f\"data/synthetic/{json_filename}\"\n",
    "    os.makedirs(os.path.dirname(json_path), exist_ok=True)\n",
    "    \n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(detailed_results, f, indent=2)\n",
    "    \n",
    "    print(f\"Saved detailed results to {json_path}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No synthetic data to save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c6c09c",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Scale up**: Process larger datasets of COVID news articles\n",
    "2. **Quality evaluation**: Manually review generated articles for quality\n",
    "3. **Experiment with prompts**: Try different prompt engineering techniques\n",
    "4. **Try different models**: Compare OpenAI vs Anthropic vs other LLMs\n",
    "5. **Move to classification**: Use the generated data to train classification models\n",
    "\n",
    "The generated synthetic data can now be used in the classification notebooks to train models that distinguish between real and fake news."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
