{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2fa4e7f",
   "metadata": {},
   "source": [
    "# Automatic Evaluation of Synthetic Data Generation\n",
    "\n",
    "This notebook implements automatic evaluation metrics for synthetic news articles and tweets.\n",
    "\n",
    "## Automatic Evaluation Metrics:\n",
    "1. **Correctness**: How accurately were facts extracted and modified?\n",
    "2. **Coherence**: How well does the synthetic content maintain logical flow?\n",
    "3. **Dissimilarity**: How different is the synthetic content from the original?\n",
    "\n",
    "## Process:\n",
    "- Load generated synthetic data\n",
    "- Apply automatic evaluation metrics\n",
    "- Generate quantitative quality scores\n",
    "- Create evaluation reports\n",
    "- Compare with manual evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54bc6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import nltk\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# For text similarity and coherence\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from textstat import flesch_reading_ease, flesch_kincaid_grade\n",
    "import spacy\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Current time: {datetime.now()}\")\n",
    "\n",
    "# Load spaCy model for advanced NLP\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"✅ SpaCy model loaded successfully\")\n",
    "except OSError:\n",
    "    print(\"⚠️ SpaCy model not found. Install with: python -m spacy download en_core_web_sm\")\n",
    "    nlp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c8781b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load generated synthetic data results\n",
    "def load_evaluation_data(results_dir: str = \"../../results\") -> Dict[str, List[Dict]]:\n",
    "    \"\"\"\n",
    "    Load generated results for automatic evaluation\n",
    "    \"\"\"\n",
    "    import glob\n",
    "    \n",
    "    data = {'news': [], 'tweets': []}\n",
    "    \n",
    "    # Load news results\n",
    "    news_files = glob.glob(f\"{results_dir}/news_batch_final_*.json\")\n",
    "    if news_files:\n",
    "        latest_news = max(news_files, key=os.path.getctime)\n",
    "        with open(latest_news, 'r') as f:\n",
    "            data['news'] = json.load(f)\n",
    "        print(f\"✅ Loaded {len(data['news'])} news results from {latest_news}\")\n",
    "    \n",
    "    # Load tweet results  \n",
    "    tweet_files = glob.glob(f\"{results_dir}/tweets_batch_final_*.json\")\n",
    "    if tweet_files:\n",
    "        latest_tweets = max(tweet_files, key=os.path.getctime)\n",
    "        with open(latest_tweets, 'r') as f:\n",
    "            data['tweets'] = json.load(f)\n",
    "        print(f\"✅ Loaded {len(data['tweets'])} tweet results from {latest_tweets}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Load data\n",
    "evaluation_data = load_evaluation_data()\n",
    "\n",
    "print(f\"\\n📊 EVALUATION DATA SUMMARY:\")\n",
    "print(f\"News articles: {len(evaluation_data['news'])}\")\n",
    "print(f\"Tweets: {len(evaluation_data['tweets'])}\")\n",
    "print(f\"Total items: {len(evaluation_data['news']) + len(evaluation_data['tweets'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecdcccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric 1: Correctness Evaluation\n",
    "class CorrectnessEvaluator:\n",
    "    \"\"\"\n",
    "    Evaluate correctness of fact extraction and modification\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    \n",
    "    def evaluate_fact_extraction_correctness(self, original_text: str, extracted_facts: List[Dict]) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate if extracted facts are actually present in the original text\n",
    "        \"\"\"\n",
    "        if not extracted_facts:\n",
    "            return 0.0\n",
    "        \n",
    "        original_lower = original_text.lower()\n",
    "        correct_extractions = 0\n",
    "        \n",
    "        for fact in extracted_facts:\n",
    "            specific_data = fact.get('specific_data', '').lower()\n",
    "            \n",
    "            if not specific_data:\n",
    "                continue\n",
    "            \n",
    "            # Check if the specific data appears in original text\n",
    "            # Handle multi-word entities\n",
    "            words = specific_data.split()\n",
    "            if len(words) == 1:\n",
    "                # Single word - exact match\n",
    "                if specific_data in original_lower:\n",
    "                    correct_extractions += 1\n",
    "            else:\n",
    "                # Multi-word - check if all words appear nearby\n",
    "                if specific_data in original_lower:\n",
    "                    correct_extractions += 1\n",
    "                else:\n",
    "                    # Check if most words appear\n",
    "                    word_matches = sum(1 for word in words if word in original_lower and word not in self.stopwords)\n",
    "                    if word_matches >= len(words) * 0.7:  # 70% of words match\n",
    "                        correct_extractions += 0.7\n",
    "        \n",
    "        return correct_extractions / len(extracted_facts)\n",
    "    \n",
    "    def evaluate_fact_modification_appropriateness(self, extracted_facts: List[Dict], modified_facts: List[Dict]) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate if fact modifications maintain the same category/type\n",
    "        \"\"\"\n",
    "        if not extracted_facts or not modified_facts:\n",
    "            return 0.0\n",
    "        \n",
    "        if len(extracted_facts) != len(modified_facts):\n",
    "            return 0.0  # Mismatch in fact count\n",
    "        \n",
    "        appropriate_modifications = 0\n",
    "        \n",
    "        for orig_fact, mod_fact in zip(extracted_facts, modified_facts):\n",
    "            # Check if fact type/name matches\n",
    "            if orig_fact.get('name_of_fact') == mod_fact.get('name_of_fact'):\n",
    "                # Check if description is maintained\n",
    "                if orig_fact.get('description_of_fact') == mod_fact.get('description_of_fact'):\n",
    "                    # Check if specific data is actually different\n",
    "                    orig_data = orig_fact.get('specific_data', '').lower()\n",
    "                    mod_data = mod_fact.get('specific_data', '').lower()\n",
    "                    \n",
    "                    if orig_data != mod_data and mod_data != '':\n",
    "                        appropriate_modifications += 1\n",
    "                    elif orig_data == mod_data:\n",
    "                        appropriate_modifications += 0.5  # Partial credit for no change\n",
    "        \n",
    "        return appropriate_modifications / len(extracted_facts)\n",
    "    \n",
    "    def evaluate_fact_replacement_accuracy(self, original_text: str, synthetic_text: str, \n",
    "                                         extracted_facts: List[Dict], modified_facts: List[Dict]) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate if facts were correctly replaced in synthetic text\n",
    "        \"\"\"\n",
    "        if not extracted_facts or not modified_facts:\n",
    "            return 0.0\n",
    "        \n",
    "        successful_replacements = 0\n",
    "        \n",
    "        for orig_fact, mod_fact in zip(extracted_facts, modified_facts):\n",
    "            orig_data = orig_fact.get('specific_data', '')\n",
    "            mod_data = mod_fact.get('specific_data', '')\n",
    "            \n",
    "            if not orig_data or not mod_data:\n",
    "                continue\n",
    "            \n",
    "            # Check if original fact is removed from synthetic text\n",
    "            orig_removed = orig_data.lower() not in synthetic_text.lower()\n",
    "            \n",
    "            # Check if modified fact is present in synthetic text\n",
    "            mod_added = mod_data.lower() in synthetic_text.lower()\n",
    "            \n",
    "            if orig_removed and mod_added:\n",
    "                successful_replacements += 1\n",
    "            elif orig_removed or mod_added:\n",
    "                successful_replacements += 0.5  # Partial success\n",
    "        \n",
    "        return successful_replacements / len(extracted_facts)\n",
    "\n",
    "print(\"Correctness evaluator defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efd19f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric 2: Coherence Evaluation\n",
    "class CoherenceEvaluator:\n",
    "    \"\"\"\n",
    "    Evaluate coherence of synthetic content\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, nlp_model=None):\n",
    "        self.nlp = nlp_model\n",
    "    \n",
    "    def evaluate_readability_coherence(self, text: str) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate readability as a proxy for coherence\n",
    "        \"\"\"\n",
    "        try:\n",
    "            flesch_score = flesch_reading_ease(text)\n",
    "            fk_grade = flesch_kincaid_grade(text)\n",
    "            \n",
    "            # Normalize Flesch score to 0-1 range (higher is better)\n",
    "            flesch_normalized = min(1.0, max(0.0, flesch_score / 100.0))\n",
    "            \n",
    "            # Normalize FK grade (lower grades are better, cap at grade 20)\n",
    "            fk_normalized = max(0.0, 1.0 - (fk_grade / 20.0))\n",
    "            \n",
    "            return {\n",
    "                'flesch_ease': flesch_score,\n",
    "                'fk_grade': fk_grade,\n",
    "                'readability_score': (flesch_normalized + fk_normalized) / 2\n",
    "            }\n",
    "        except:\n",
    "            return {'flesch_ease': 0, 'fk_grade': 20, 'readability_score': 0.0}\n",
    "    \n",
    "    def evaluate_sentence_coherence(self, text: str) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate coherence based on sentence structure and flow\n",
    "        \"\"\"\n",
    "        if not self.nlp:\n",
    "            return 0.5  # Neutral score if spaCy not available\n",
    "        \n",
    "        doc = self.nlp(text)\n",
    "        sentences = list(doc.sents)\n",
    "        \n",
    "        if len(sentences) < 2:\n",
    "            return 1.0  # Single sentence is coherent by default\n",
    "        \n",
    "        coherence_score = 0.0\n",
    "        \n",
    "        # Check for sentence transition coherence\n",
    "        for i in range(len(sentences) - 1):\n",
    "            sent1 = sentences[i]\n",
    "            sent2 = sentences[i + 1]\n",
    "            \n",
    "            # Simple coherence checks\n",
    "            # 1. Sentence length variation (not all very short or very long)\n",
    "            len_ratio = min(len(sent1.text), len(sent2.text)) / max(len(sent1.text), len(sent2.text))\n",
    "            length_score = min(1.0, len_ratio + 0.3)  # Penalty for extreme length differences\n",
    "            \n",
    "            # 2. Entity continuity (shared entities between sentences)\n",
    "            ents1 = set(ent.text.lower() for ent in sent1.ents)\n",
    "            ents2 = set(ent.text.lower() for ent in sent2.ents)\n",
    "            \n",
    "            if ents1 and ents2:\n",
    "                entity_overlap = len(ents1.intersection(ents2)) / len(ents1.union(ents2))\n",
    "            else:\n",
    "                entity_overlap = 0.3  # Neutral score\n",
    "            \n",
    "            # 3. Lexical cohesion (shared content words)\n",
    "            words1 = set(token.lemma_.lower() for token in sent1 \n",
    "                        if not token.is_stop and not token.is_punct and token.is_alpha)\n",
    "            words2 = set(token.lemma_.lower() for token in sent2 \n",
    "                        if not token.is_stop and not token.is_punct and token.is_alpha)\n",
    "            \n",
    "            if words1 and words2:\n",
    "                lexical_overlap = len(words1.intersection(words2)) / len(words1.union(words2))\n",
    "            else:\n",
    "                lexical_overlap = 0.0\n",
    "            \n",
    "            # Combine scores\n",
    "            sentence_coherence = (length_score + entity_overlap + lexical_overlap) / 3\n",
    "            coherence_score += sentence_coherence\n",
    "        \n",
    "        return coherence_score / (len(sentences) - 1)\n",
    "    \n",
    "    def evaluate_semantic_coherence(self, original_text: str, synthetic_text: str) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate if synthetic text maintains semantic coherence with original structure\n",
    "        \"\"\"\n",
    "        # Use TF-IDF to compare semantic similarity at document level\n",
    "        vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "        \n",
    "        try:\n",
    "            tfidf_matrix = vectorizer.fit_transform([original_text, synthetic_text])\n",
    "            similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "            \n",
    "            # We want some similarity (coherent structure) but not too much (should be different)\n",
    "            # Optimal range: 0.3-0.7 similarity\n",
    "            if 0.3 <= similarity <= 0.7:\n",
    "                coherence_score = 1.0\n",
    "            elif similarity < 0.3:\n",
    "                coherence_score = similarity / 0.3  # Penalty for too much difference\n",
    "            else:  # similarity > 0.7\n",
    "                coherence_score = (1.0 - similarity) / 0.3  # Penalty for too much similarity\n",
    "            \n",
    "            return max(0.0, min(1.0, coherence_score))\n",
    "        except:\n",
    "            return 0.5  # Neutral score on error\n",
    "\n",
    "print(\"Coherence evaluator defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74fa8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric 3: Dissimilarity Evaluation\n",
    "class DissimilarityEvaluator:\n",
    "    \"\"\"\n",
    "    Evaluate dissimilarity between original and synthetic content\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def evaluate_lexical_dissimilarity(self, original_text: str, synthetic_text: str) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate lexical dissimilarity using TF-IDF cosine distance\n",
    "        \"\"\"\n",
    "        vectorizer = TfidfVectorizer(stop_words='english', max_features=1000, ngram_range=(1, 2))\n",
    "        \n",
    "        try:\n",
    "            tfidf_matrix = vectorizer.fit_transform([original_text, synthetic_text])\n",
    "            similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "            dissimilarity = 1.0 - similarity\n",
    "            return max(0.0, min(1.0, dissimilarity))\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def evaluate_structural_dissimilarity(self, original_text: str, synthetic_text: str) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate structural differences (length, sentence count, etc.)\n",
    "        \"\"\"\n",
    "        orig_sentences = nltk.sent_tokenize(original_text)\n",
    "        synth_sentences = nltk.sent_tokenize(synthetic_text)\n",
    "        \n",
    "        orig_words = nltk.word_tokenize(original_text)\n",
    "        synth_words = nltk.word_tokenize(synthetic_text)\n",
    "        \n",
    "        # Length difference (normalized)\n",
    "        length_diff = abs(len(orig_words) - len(synth_words)) / max(len(orig_words), len(synth_words))\n",
    "        \n",
    "        # Sentence count difference (normalized)\n",
    "        sent_diff = abs(len(orig_sentences) - len(synth_sentences)) / max(len(orig_sentences), len(synth_sentences))\n",
    "        \n",
    "        # Average sentence length difference\n",
    "        orig_avg_sent_len = len(orig_words) / len(orig_sentences) if orig_sentences else 0\n",
    "        synth_avg_sent_len = len(synth_words) / len(synth_sentences) if synth_sentences else 0\n",
    "        \n",
    "        if orig_avg_sent_len > 0:\n",
    "            sent_len_diff = abs(orig_avg_sent_len - synth_avg_sent_len) / orig_avg_sent_len\n",
    "        else:\n",
    "            sent_len_diff = 0\n",
    "        \n",
    "        # Combine structural differences\n",
    "        structural_dissimilarity = (length_diff + sent_diff + sent_len_diff) / 3\n",
    "        return min(1.0, structural_dissimilarity)\n",
    "    \n",
    "    def evaluate_entity_dissimilarity(self, original_text: str, synthetic_text: str, nlp_model=None) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate dissimilarity in named entities\n",
    "        \"\"\"\n",
    "        if not nlp_model:\n",
    "            # Fallback: simple word-level comparison\n",
    "            orig_words = set(word.lower() for word in nltk.word_tokenize(original_text) if word.isalpha())\n",
    "            synth_words = set(word.lower() for word in nltk.word_tokenize(synthetic_text) if word.isalpha())\n",
    "            \n",
    "            if not orig_words:\n",
    "                return 0.0\n",
    "            \n",
    "            overlap = len(orig_words.intersection(synth_words))\n",
    "            return 1.0 - (overlap / len(orig_words))\n",
    "        \n",
    "        # Use spaCy for entity extraction\n",
    "        orig_doc = nlp_model(original_text)\n",
    "        synth_doc = nlp_model(synthetic_text)\n",
    "        \n",
    "        orig_entities = set(ent.text.lower() for ent in orig_doc.ents)\n",
    "        synth_entities = set(ent.text.lower() for ent in synth_doc.ents)\n",
    "        \n",
    "        if not orig_entities:\n",
    "            return 1.0 if synth_entities else 0.0\n",
    "        \n",
    "        overlap = len(orig_entities.intersection(synth_entities))\n",
    "        entity_dissimilarity = 1.0 - (overlap / len(orig_entities))\n",
    "        \n",
    "        return entity_dissimilarity\n",
    "    \n",
    "    def evaluate_fact_dissimilarity(self, extracted_facts: List[Dict], modified_facts: List[Dict]) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate how different the modified facts are from extracted facts\n",
    "        \"\"\"\n",
    "        if not extracted_facts or not modified_facts:\n",
    "            return 0.0\n",
    "        \n",
    "        total_dissimilarity = 0.0\n",
    "        \n",
    "        for orig_fact, mod_fact in zip(extracted_facts, modified_facts):\n",
    "            orig_data = orig_fact.get('specific_data', '').lower()\n",
    "            mod_data = mod_fact.get('specific_data', '').lower()\n",
    "            \n",
    "            if not orig_data or not mod_data:\n",
    "                continue\n",
    "            \n",
    "            # Simple string dissimilarity\n",
    "            if orig_data == mod_data:\n",
    "                dissimilarity = 0.0  # No change\n",
    "            else:\n",
    "                # Calculate character-level dissimilarity\n",
    "                max_len = max(len(orig_data), len(mod_data))\n",
    "                if max_len == 0:\n",
    "                    dissimilarity = 0.0\n",
    "                else:\n",
    "                    # Simple edit distance approximation\n",
    "                    common_chars = sum(1 for c in orig_data if c in mod_data)\n",
    "                    dissimilarity = 1.0 - (common_chars / max_len)\n",
    "            \n",
    "            total_dissimilarity += dissimilarity\n",
    "        \n",
    "        return total_dissimilarity / len(extracted_facts)\n",
    "\n",
    "print(\"Dissimilarity evaluator defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ee6f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive evaluation function\n",
    "def evaluate_synthetic_data_item(item: Dict, \n",
    "                                correctness_eval: CorrectnessEvaluator,\n",
    "                                coherence_eval: CoherenceEvaluator,\n",
    "                                dissimilarity_eval: DissimilarityEvaluator) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate a single synthetic data item across all metrics\n",
    "    \"\"\"\n",
    "    original_text = item.get('original_content', '')\n",
    "    synthetic_text = item.get('synthetic_content', '')\n",
    "    extracted_facts = item.get('extracted_facts', [])\n",
    "    modified_facts = item.get('modified_facts', [])\n",
    "    \n",
    "    # Correctness metrics\n",
    "    fact_extraction_correctness = correctness_eval.evaluate_fact_extraction_correctness(original_text, extracted_facts)\n",
    "    fact_modification_appropriateness = correctness_eval.evaluate_fact_modification_appropriateness(extracted_facts, modified_facts)\n",
    "    fact_replacement_accuracy = correctness_eval.evaluate_fact_replacement_accuracy(original_text, synthetic_text, extracted_facts, modified_facts)\n",
    "    \n",
    "    correctness_score = (fact_extraction_correctness + fact_modification_appropriateness + fact_replacement_accuracy) / 3\n",
    "    \n",
    "    # Coherence metrics\n",
    "    readability_metrics = coherence_eval.evaluate_readability_coherence(synthetic_text)\n",
    "    sentence_coherence = coherence_eval.evaluate_sentence_coherence(synthetic_text)\n",
    "    semantic_coherence = coherence_eval.evaluate_semantic_coherence(original_text, synthetic_text)\n",
    "    \n",
    "    coherence_score = (readability_metrics['readability_score'] + sentence_coherence + semantic_coherence) / 3\n",
    "    \n",
    "    # Dissimilarity metrics\n",
    "    lexical_dissimilarity = dissimilarity_eval.evaluate_lexical_dissimilarity(original_text, synthetic_text)\n",
    "    structural_dissimilarity = dissimilarity_eval.evaluate_structural_dissimilarity(original_text, synthetic_text)\n",
    "    entity_dissimilarity = dissimilarity_eval.evaluate_entity_dissimilarity(original_text, synthetic_text, nlp)\n",
    "    fact_dissimilarity = dissimilarity_eval.evaluate_fact_dissimilarity(extracted_facts, modified_facts)\n",
    "    \n",
    "    dissimilarity_score = (lexical_dissimilarity + structural_dissimilarity + entity_dissimilarity + fact_dissimilarity) / 4\n",
    "    \n",
    "    # Overall quality score\n",
    "    overall_score = (correctness_score + coherence_score + dissimilarity_score) / 3\n",
    "    \n",
    "    return {\n",
    "        'evaluation_id': item.get('generation_info', {}).get('index', 0),\n",
    "        'content_type': item.get('generation_info', {}).get('content_type', 'unknown'),\n",
    "        \n",
    "        # Correctness components\n",
    "        'fact_extraction_correctness': fact_extraction_correctness,\n",
    "        'fact_modification_appropriateness': fact_modification_appropriateness,\n",
    "        'fact_replacement_accuracy': fact_replacement_accuracy,\n",
    "        'correctness_score': correctness_score,\n",
    "        \n",
    "        # Coherence components\n",
    "        'readability_score': readability_metrics['readability_score'],\n",
    "        'sentence_coherence': sentence_coherence,\n",
    "        'semantic_coherence': semantic_coherence,\n",
    "        'coherence_score': coherence_score,\n",
    "        \n",
    "        # Dissimilarity components\n",
    "        'lexical_dissimilarity': lexical_dissimilarity,\n",
    "        'structural_dissimilarity': structural_dissimilarity,\n",
    "        'entity_dissimilarity': entity_dissimilarity,\n",
    "        'fact_dissimilarity': fact_dissimilarity,\n",
    "        'dissimilarity_score': dissimilarity_score,\n",
    "        \n",
    "        # Overall\n",
    "        'overall_quality_score': overall_score,\n",
    "        \n",
    "        # Additional metadata\n",
    "        'flesch_reading_ease': readability_metrics['flesch_ease'],\n",
    "        'flesch_kincaid_grade': readability_metrics['fk_grade'],\n",
    "        'num_extracted_facts': len(extracted_facts),\n",
    "        'num_modified_facts': len(modified_facts)\n",
    "    }\n",
    "\n",
    "print(\"Comprehensive evaluation function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd951439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run automatic evaluation on all data\n",
    "print(\"🔄 RUNNING AUTOMATIC EVALUATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Initialize evaluators\n",
    "correctness_evaluator = CorrectnessEvaluator()\n",
    "coherence_evaluator = CoherenceEvaluator(nlp)\n",
    "dissimilarity_evaluator = DissimilarityEvaluator()\n",
    "\n",
    "all_evaluations = []\n",
    "\n",
    "# Evaluate news articles\n",
    "if evaluation_data['news']:\n",
    "    print(f\"\\n📰 Evaluating {len(evaluation_data['news'])} news articles...\")\n",
    "    \n",
    "    for i, item in enumerate(evaluation_data['news']):\n",
    "        if i % 10 == 0:\n",
    "            print(f\"  Progress: {i}/{len(evaluation_data['news'])}\")\n",
    "        \n",
    "        evaluation = evaluate_synthetic_data_item(item, correctness_evaluator, coherence_evaluator, dissimilarity_evaluator)\n",
    "        all_evaluations.append(evaluation)\n",
    "\n",
    "# Evaluate tweets\n",
    "if evaluation_data['tweets']:\n",
    "    print(f\"\\n🐦 Evaluating {len(evaluation_data['tweets'])} tweets...\")\n",
    "    \n",
    "    for i, item in enumerate(evaluation_data['tweets']):\n",
    "        if i % 10 == 0:\n",
    "            print(f\"  Progress: {i}/{len(evaluation_data['tweets'])}\")\n",
    "        \n",
    "        evaluation = evaluate_synthetic_data_item(item, correctness_evaluator, coherence_evaluator, dissimilarity_evaluator)\n",
    "        all_evaluations.append(evaluation)\n",
    "\n",
    "print(f\"\\n✅ Automatic evaluation completed!\")\n",
    "print(f\"Total items evaluated: {len(all_evaluations)}\")\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "evaluation_df = pd.DataFrame(all_evaluations)\n",
    "print(f\"\\n📊 Evaluation results shape: {evaluation_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799971cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate evaluation report and visualizations\n",
    "print(\"📈 AUTOMATIC EVALUATION RESULTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Overall statistics\n",
    "print(\"\\n🎯 OVERALL QUALITY SCORES:\")\n",
    "print(f\"Mean Overall Quality: {evaluation_df['overall_quality_score'].mean():.3f}\")\n",
    "print(f\"Median Overall Quality: {evaluation_df['overall_quality_score'].median():.3f}\")\n",
    "print(f\"Std Overall Quality: {evaluation_df['overall_quality_score'].std():.3f}\")\n",
    "\n",
    "# Metric breakdown\n",
    "print(\"\\n📋 METRIC BREAKDOWN:\")\n",
    "metric_cols = ['correctness_score', 'coherence_score', 'dissimilarity_score']\n",
    "for metric in metric_cols:\n",
    "    mean_score = evaluation_df[metric].mean()\n",
    "    print(f\"{metric.replace('_', ' ').title()}: {mean_score:.3f}\")\n",
    "\n",
    "# Content type comparison\n",
    "if 'content_type' in evaluation_df.columns:\n",
    "    print(\"\\n📊 BY CONTENT TYPE:\")\n",
    "    content_type_stats = evaluation_df.groupby('content_type')['overall_quality_score'].agg(['mean', 'median', 'std', 'count'])\n",
    "    print(content_type_stats)\n",
    "\n",
    "# Create visualizations\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# 1. Overall quality distribution\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.hist(evaluation_df['overall_quality_score'], bins=20, alpha=0.7, edgecolor='black')\n",
    "plt.title('Overall Quality Score Distribution')\n",
    "plt.xlabel('Quality Score')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# 2. Metric comparison\n",
    "plt.subplot(2, 3, 2)\n",
    "metric_means = [evaluation_df[col].mean() for col in metric_cols]\n",
    "metric_names = [col.replace('_score', '').title() for col in metric_cols]\n",
    "plt.bar(metric_names, metric_means, alpha=0.7)\n",
    "plt.title('Average Scores by Metric')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# 3. Content type comparison (if available)\n",
    "if 'content_type' in evaluation_df.columns and evaluation_df['content_type'].nunique() > 1:\n",
    "    plt.subplot(2, 3, 3)\n",
    "    sns.boxplot(data=evaluation_df, x='content_type', y='overall_quality_score')\n",
    "    plt.title('Quality by Content Type')\n",
    "    plt.ylabel('Overall Quality Score')\n",
    "\n",
    "# 4. Correlation heatmap\n",
    "plt.subplot(2, 3, 4)\n",
    "correlation_cols = ['correctness_score', 'coherence_score', 'dissimilarity_score', 'overall_quality_score']\n",
    "corr_matrix = evaluation_df[correlation_cols].corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Metric Correlations')\n",
    "\n",
    "# 5. Detailed metric breakdown\n",
    "plt.subplot(2, 3, 5)\n",
    "detailed_metrics = ['fact_extraction_correctness', 'fact_modification_appropriateness', 'fact_replacement_accuracy']\n",
    "detailed_means = [evaluation_df[col].mean() for col in detailed_metrics]\n",
    "detailed_names = [col.replace('_', ' ').replace('fact ', '').title() for col in detailed_metrics]\n",
    "plt.bar(detailed_names, detailed_means, alpha=0.7)\n",
    "plt.title('Correctness Components')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# 6. Quality vs number of facts\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.scatter(evaluation_df['num_extracted_facts'], evaluation_df['overall_quality_score'], alpha=0.6)\n",
    "plt.xlabel('Number of Extracted Facts')\n",
    "plt.ylabel('Overall Quality Score')\n",
    "plt.title('Quality vs Number of Facts')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save evaluation results\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "results_file = f\"../../evaluation/automatic_evaluation_results_{timestamp}.csv\"\n",
    "os.makedirs(\"../../evaluation\", exist_ok=True)\n",
    "\n",
    "evaluation_df.to_csv(results_file, index=False)\n",
    "print(f\"\\n💾 Evaluation results saved to: {results_file}\")\n",
    "\n",
    "# Quality thresholds and recommendations\n",
    "print(\"\\n🎯 QUALITY ASSESSMENT & RECOMMENDATIONS:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "mean_overall = evaluation_df['overall_quality_score'].mean()\n",
    "mean_correctness = evaluation_df['correctness_score'].mean()\n",
    "mean_coherence = evaluation_df['coherence_score'].mean()\n",
    "mean_dissimilarity = evaluation_df['dissimilarity_score'].mean()\n",
    "\n",
    "if mean_overall >= 0.7:\n",
    "    print(\"✅ EXCELLENT: Overall quality is high - proceed with full dataset\")\n",
    "elif mean_overall >= 0.6:\n",
    "    print(\"✅ GOOD: Quality is acceptable - proceed with caution\")\n",
    "elif mean_overall >= 0.5:\n",
    "    print(\"⚠️ MODERATE: Quality needs improvement before scaling\")\n",
    "else:\n",
    "    print(\"❌ POOR: Significant improvements needed\")\n",
    "\n",
    "print(\"\\nSpecific recommendations:\")\n",
    "if mean_correctness < 0.6:\n",
    "    print(\"• Improve fact extraction and modification accuracy\")\n",
    "if mean_coherence < 0.6:\n",
    "    print(\"• Review and improve text generation for better coherence\")\n",
    "if mean_dissimilarity < 0.4:\n",
    "    print(\"• Increase dissimilarity - synthetic content too similar to original\")\n",
    "elif mean_dissimilarity > 0.8:\n",
    "    print(\"• Decrease dissimilarity - synthetic content too different from original\")\n",
    "\n",
    "print(f\"\\n📊 Items with quality score >= 0.7: {(evaluation_df['overall_quality_score'] >= 0.7).sum()}/{len(evaluation_df)} ({(evaluation_df['overall_quality_score'] >= 0.7).mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ac9f66",
   "metadata": {},
   "source": [
    "## Evaluation Summary\n",
    "\n",
    "This notebook provides comprehensive automatic evaluation with three key metrics:\n",
    "\n",
    "### 1. Correctness (33% weight)\n",
    "- **Fact Extraction Correctness**: Are extracted facts present in original text?\n",
    "- **Fact Modification Appropriateness**: Are modifications realistic and type-consistent?\n",
    "- **Fact Replacement Accuracy**: Are facts correctly replaced in synthetic text?\n",
    "\n",
    "### 2. Coherence (33% weight)\n",
    "- **Readability**: Flesch reading ease and grade level\n",
    "- **Sentence Coherence**: Logical flow between sentences\n",
    "- **Semantic Coherence**: Maintaining overall meaning structure\n",
    "\n",
    "### 3. Dissimilarity (33% weight)\n",
    "- **Lexical Dissimilarity**: TF-IDF cosine distance\n",
    "- **Structural Dissimilarity**: Length and sentence structure differences\n",
    "- **Entity Dissimilarity**: Named entity changes\n",
    "- **Fact Dissimilarity**: How different modified facts are from originals\n",
    "\n",
    "### Quality Thresholds:\n",
    "- **>= 0.7**: Excellent quality - proceed with full dataset\n",
    "- **0.6-0.7**: Good quality - proceed with monitoring\n",
    "- **0.5-0.6**: Moderate quality - improvements recommended\n",
    "- **< 0.5**: Poor quality - significant changes needed\n",
    "\n",
    "### Next Steps:\n",
    "1. Compare automatic scores with manual evaluation results\n",
    "2. Identify patterns in high/low quality items\n",
    "3. Use insights to improve fact schemas and generation process\n",
    "4. Proceed to classification training if scores are satisfactory"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
