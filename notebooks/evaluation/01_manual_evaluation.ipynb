{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aa07ebf",
   "metadata": {},
   "source": [
    "# Manual Evaluation of Synthetic Data Generation\n",
    "\n",
    "This notebook is designed for manual evaluation of generated synthetic news articles and tweets.\n",
    "\n",
    "## Evaluation Process:\n",
    "1. **Sample Selection**: 100-300 articles/tweets for manual evaluation\n",
    "2. **Multi-Annotator Setup**: At least 3 annotators per item\n",
    "3. **Evaluation Criteria**: \n",
    "   - **Inappropriate**: Fact extraction/modification is clearly wrong\n",
    "   - **Appropriate**: Fact extraction/modification is correct and plausible\n",
    "   - **In-between**: Partially correct or ambiguous\n",
    "4. **Agreement Analysis**: Calculate inter-annotator agreement\n",
    "5. **Decision Making**: Determine if quality is sufficient to proceed\n",
    "\n",
    "## Output:\n",
    "- Annotated evaluation dataset\n",
    "- Inter-annotator agreement scores\n",
    "- Quality assessment recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d8465b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Any\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "# For inter-annotator agreement\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import itertools\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Current time: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d74ac18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load generated synthetic data results\n",
    "print(\"üìÅ LOADING GENERATED RESULTS\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Load your generated results - adjust file paths as needed\n",
    "results_files = {\n",
    "    'news': '../../results/news_batch_final_*.json',  # Replace with actual filename\n",
    "    'tweets': '../../results/tweets_batch_final_*.json'  # Replace with actual filename\n",
    "}\n",
    "\n",
    "# Function to load results\n",
    "def load_generated_results(file_pattern: str, content_type: str) -> List[Dict]:\n",
    "    import glob\n",
    "    files = glob.glob(file_pattern)\n",
    "    \n",
    "    if not files:\n",
    "        print(f\"‚ö†Ô∏è No {content_type} results found matching pattern: {file_pattern}\")\n",
    "        return []\n",
    "    \n",
    "    # Use the most recent file\n",
    "    latest_file = max(files, key=os.path.getctime)\n",
    "    print(f\"Loading {content_type} from: {latest_file}\")\n",
    "    \n",
    "    with open(latest_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(data)} {content_type} results\")\n",
    "    return data\n",
    "\n",
    "# Load results\n",
    "news_results = load_generated_results(results_files['news'], 'news')\n",
    "tweet_results = load_generated_results(results_files['tweets'], 'tweets')\n",
    "\n",
    "print(f\"\\nüìä TOTAL RESULTS LOADED:\")\n",
    "print(f\"News articles: {len(news_results)}\")\n",
    "print(f\"Tweets: {len(tweet_results)}\")\n",
    "print(f\"Total items: {len(news_results) + len(tweet_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fde5720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample selection for manual evaluation\n",
    "def create_evaluation_sample(results: List[Dict], \n",
    "                           content_type: str, \n",
    "                           sample_size: int = 100,\n",
    "                           random_seed: int = 42) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Create a sample for manual evaluation\n",
    "    \"\"\"\n",
    "    random.seed(random_seed)\n",
    "    \n",
    "    if len(results) <= sample_size:\n",
    "        print(f\"üìä Using all {len(results)} {content_type} (less than requested {sample_size})\")\n",
    "        sample = results.copy()\n",
    "    else:\n",
    "        print(f\"üìä Sampling {sample_size} {content_type} from {len(results)} total\")\n",
    "        sample = random.sample(results, sample_size)\n",
    "    \n",
    "    # Add evaluation structure to each item\n",
    "    for i, item in enumerate(sample):\n",
    "        item['evaluation_id'] = f\"{content_type}_{i+1:03d}\"\n",
    "        item['content_type'] = content_type\n",
    "        \n",
    "        # Initialize evaluation fields\n",
    "        item['manual_evaluation'] = {\n",
    "            'annotator_1': {'fact_extraction': None, 'fact_modification': None, 'notes': ''},\n",
    "            'annotator_2': {'fact_extraction': None, 'fact_modification': None, 'notes': ''},\n",
    "            'annotator_3': {'fact_extraction': None, 'fact_modification': None, 'notes': ''},\n",
    "            'consensus': {'fact_extraction': None, 'fact_modification': None, 'notes': ''}\n",
    "        }\n",
    "    \n",
    "    return sample\n",
    "\n",
    "# Configuration for sampling\n",
    "SAMPLE_SIZE_NEWS = 100  # Adjust as needed (100-300)\n",
    "SAMPLE_SIZE_TWEETS = 100  # Adjust as needed (100-300)\n",
    "\n",
    "# Create evaluation samples\n",
    "news_sample = create_evaluation_sample(news_results, 'news', SAMPLE_SIZE_NEWS)\n",
    "tweet_sample = create_evaluation_sample(tweet_results, 'tweets', SAMPLE_SIZE_TWEETS)\n",
    "\n",
    "print(f\"\\n‚úÖ EVALUATION SAMPLES CREATED:\")\n",
    "print(f\"News sample: {len(news_sample)} items\")\n",
    "print(f\"Tweet sample: {len(tweet_sample)} items\")\n",
    "print(f\"Total for evaluation: {len(news_sample) + len(tweet_sample)} items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b314da06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluation templates for annotators\n",
    "def create_annotation_template(sample: List[Dict], content_type: str) -> None:\n",
    "    \"\"\"\n",
    "    Create human-readable templates for manual annotation\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Create evaluation directory\n",
    "    eval_dir = \"../../evaluation\"\n",
    "    os.makedirs(eval_dir, exist_ok=True)\n",
    "    \n",
    "    # Create detailed evaluation file for annotators\n",
    "    eval_file = f\"{eval_dir}/{content_type}_manual_evaluation_{timestamp}.json\"\n",
    "    \n",
    "    # Create simplified version for easier annotation\n",
    "    simplified_items = []\n",
    "    \n",
    "    for item in sample:\n",
    "        simplified_item = {\n",
    "            'evaluation_id': item['evaluation_id'],\n",
    "            'content_type': content_type,\n",
    "            'original_content': item['original_content'][:500] + \"...\" if len(item['original_content']) > 500 else item['original_content'],\n",
    "            'extracted_facts': item['extracted_facts'],\n",
    "            'modified_facts': item['modified_facts'],\n",
    "            'synthetic_content': item['synthetic_content'][:500] + \"...\" if len(item['synthetic_content']) > 500 else item['synthetic_content'],\n",
    "            'evaluation_template': {\n",
    "                'fact_extraction_quality': {\n",
    "                    'rating': None,  # 'appropriate', 'inappropriate', 'in-between'\n",
    "                    'explanation': '',\n",
    "                    'specific_issues': []\n",
    "                },\n",
    "                'fact_modification_quality': {\n",
    "                    'rating': None,  # 'appropriate', 'inappropriate', 'in-between'\n",
    "                    'explanation': '',\n",
    "                    'specific_issues': []\n",
    "                },\n",
    "                'overall_notes': ''\n",
    "            }\n",
    "        }\n",
    "        simplified_items.append(simplified_item)\n",
    "    \n",
    "    # Save evaluation template\n",
    "    with open(eval_file, 'w') as f:\n",
    "        json.dump(simplified_items, f, indent=2)\n",
    "    \n",
    "    print(f\"üìù Created evaluation template: {eval_file}\")\n",
    "    \n",
    "    # Create instruction file\n",
    "    instructions_file = f\"{eval_dir}/{content_type}_annotation_instructions_{timestamp}.md\"\n",
    "    \n",
    "    instructions = f\"\"\"# Manual Evaluation Instructions - {content_type.title()}\n",
    "\n",
    "## Task Overview\n",
    "You are evaluating the quality of synthetic {content_type} generation with focus on:\n",
    "1. **Fact Extraction Quality**: How well were facts extracted from original content?\n",
    "2. **Fact Modification Quality**: How appropriately were facts modified?\n",
    "\n",
    "## Rating Scale\n",
    "- **Appropriate**: Extraction/modification is correct, plausible, and maintains coherence\n",
    "- **Inappropriate**: Extraction/modification is clearly wrong, implausible, or breaks coherence\n",
    "- **In-between**: Partially correct, ambiguous, or minor issues\n",
    "\n",
    "## Evaluation Criteria\n",
    "\n",
    "### Fact Extraction Quality\n",
    "- Are the extracted facts actually present in the original content?\n",
    "- Are the fact types (name_of_fact) appropriate?\n",
    "- Are the descriptions accurate?\n",
    "- Is the specific_data correctly identified?\n",
    "\n",
    "### Fact Modification Quality\n",
    "- Are the modified facts plausible alternatives?\n",
    "- Do modifications maintain the same fact type/category?\n",
    "- Are the changes realistic and believable?\n",
    "- Do modifications create coherent misinformation?\n",
    "\n",
    "## Instructions\n",
    "1. Review each item in: `{eval_file}`\n",
    "2. For each item, fill in the `evaluation_template` section:\n",
    "   - Set `rating` to 'appropriate', 'inappropriate', or 'in-between'\n",
    "   - Provide detailed `explanation` for your rating\n",
    "   - List any `specific_issues` you identify\n",
    "   - Add `overall_notes` with additional observations\n",
    "\n",
    "## Return Instructions\n",
    "Save your completed evaluations as:\n",
    "`{content_type}_annotator_[YOUR_NAME]_{timestamp}.json`\n",
    "\n",
    "Total items to evaluate: {len(simplified_items)}\n",
    "Estimated time: {len(simplified_items) * 2} minutes (2 min per item)\n",
    "\"\"\"\n",
    "    \n",
    "    with open(instructions_file, 'w') as f:\n",
    "        f.write(instructions)\n",
    "    \n",
    "    print(f\"üìã Created instructions: {instructions_file}\")\n",
    "    \n",
    "    return eval_file, instructions_file\n",
    "\n",
    "# Create evaluation templates\n",
    "if news_sample:\n",
    "    news_eval_file, news_instructions = create_annotation_template(news_sample, 'news')\n",
    "\n",
    "if tweet_sample:\n",
    "    tweet_eval_file, tweet_instructions = create_annotation_template(tweet_sample, 'tweets')\n",
    "\n",
    "print(f\"\\n‚úÖ EVALUATION SETUP COMPLETE\")\n",
    "print(f\"üìÅ Files created in ../../evaluation/ directory\")\n",
    "print(f\"üìß Share evaluation files with your 3+ annotators\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124288f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for analyzing annotator agreement (run after getting annotations back)\n",
    "def load_annotator_evaluations(eval_dir: str, content_type: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Load completed evaluations from multiple annotators\n",
    "    \"\"\"\n",
    "    import glob\n",
    "    \n",
    "    pattern = f\"{eval_dir}/{content_type}_annotator_*.json\"\n",
    "    annotation_files = glob.glob(pattern)\n",
    "    \n",
    "    if not annotation_files:\n",
    "        print(f\"‚ö†Ô∏è No annotation files found for {content_type}\")\n",
    "        return {}\n",
    "    \n",
    "    annotations = {}\n",
    "    for file_path in annotation_files:\n",
    "        annotator_name = os.path.basename(file_path).split('_')[2]  # Extract annotator name\n",
    "        \n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        annotations[annotator_name] = data\n",
    "        print(f\"‚úÖ Loaded annotations from {annotator_name}: {len(data)} items\")\n",
    "    \n",
    "    return annotations\n",
    "\n",
    "def calculate_agreement_scores(annotations: Dict, metric: str = 'fact_extraction_quality') -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate inter-annotator agreement using Cohen's Kappa\n",
    "    \"\"\"\n",
    "    annotators = list(annotations.keys())\n",
    "    \n",
    "    if len(annotators) < 2:\n",
    "        print(\"‚ö†Ô∏è Need at least 2 annotators for agreement calculation\")\n",
    "        return {}\n",
    "    \n",
    "    # Create rating matrix\n",
    "    ratings_matrix = {}\n",
    "    \n",
    "    # Get all evaluation IDs\n",
    "    eval_ids = set()\n",
    "    for annotator_data in annotations.values():\n",
    "        for item in annotator_data:\n",
    "            eval_ids.add(item['evaluation_id'])\n",
    "    \n",
    "    eval_ids = sorted(list(eval_ids))\n",
    "    \n",
    "    # Build rating matrix\n",
    "    for annotator in annotators:\n",
    "        ratings_matrix[annotator] = []\n",
    "        \n",
    "        # Create lookup for this annotator\n",
    "        annotator_ratings = {}\n",
    "        for item in annotations[annotator]:\n",
    "            rating = item['evaluation_template'][metric]['rating']\n",
    "            if rating:  # Only include non-None ratings\n",
    "                annotator_ratings[item['evaluation_id']] = rating\n",
    "        \n",
    "        # Add ratings in order\n",
    "        for eval_id in eval_ids:\n",
    "            if eval_id in annotator_ratings:\n",
    "                ratings_matrix[annotator].append(annotator_ratings[eval_id])\n",
    "            else:\n",
    "                ratings_matrix[annotator].append(None)  # Missing rating\n",
    "    \n",
    "    # Calculate pairwise agreements\n",
    "    agreement_scores = {}\n",
    "    \n",
    "    for i, annotator1 in enumerate(annotators):\n",
    "        for j, annotator2 in enumerate(annotators[i+1:], i+1):\n",
    "            # Get paired ratings (exclude None values)\n",
    "            paired_ratings = []\n",
    "            for k in range(len(eval_ids)):\n",
    "                if (ratings_matrix[annotator1][k] is not None and \n",
    "                    ratings_matrix[annotator2][k] is not None):\n",
    "                    paired_ratings.append((\n",
    "                        ratings_matrix[annotator1][k],\n",
    "                        ratings_matrix[annotator2][k]\n",
    "                    ))\n",
    "            \n",
    "            if len(paired_ratings) > 0:\n",
    "                ratings1, ratings2 = zip(*paired_ratings)\n",
    "                kappa = cohen_kappa_score(ratings1, ratings2)\n",
    "                agreement_scores[f\"{annotator1}_vs_{annotator2}\"] = {\n",
    "                    'kappa': kappa,\n",
    "                    'items_compared': len(paired_ratings),\n",
    "                    'interpretation': interpret_kappa(kappa)\n",
    "                }\n",
    "    \n",
    "    return agreement_scores\n",
    "\n",
    "def interpret_kappa(kappa: float) -> str:\n",
    "    \"\"\"\n",
    "    Interpret Cohen's Kappa score\n",
    "    \"\"\"\n",
    "    if kappa < 0:\n",
    "        return \"Poor (worse than random)\"\n",
    "    elif kappa < 0.20:\n",
    "        return \"Slight agreement\"\n",
    "    elif kappa < 0.40:\n",
    "        return \"Fair agreement\" \n",
    "    elif kappa < 0.60:\n",
    "        return \"Moderate agreement\"\n",
    "    elif kappa < 0.80:\n",
    "        return \"Substantial agreement\"\n",
    "    else:\n",
    "        return \"Almost perfect agreement\"\n",
    "\n",
    "print(\"Agreement analysis functions defined!\")\n",
    "print(\"\\nüìã NEXT STEPS:\")\n",
    "print(\"1. Share evaluation files with 3+ annotators\")\n",
    "print(\"2. Collect completed annotation files\")\n",
    "print(\"3. Run agreement analysis in next cell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa3c729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell after collecting annotations from annotators\n",
    "\"\"\"\n",
    "# Load completed annotations\n",
    "eval_dir = \"../../evaluation\"\n",
    "\n",
    "# Load news annotations\n",
    "news_annotations = load_annotator_evaluations(eval_dir, 'news')\n",
    "tweet_annotations = load_annotator_evaluations(eval_dir, 'tweets')\n",
    "\n",
    "# Calculate agreement scores\n",
    "if news_annotations:\n",
    "    print(\"üìä NEWS EVALUATION AGREEMENT ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Fact extraction agreement\n",
    "    extraction_agreement = calculate_agreement_scores(news_annotations, 'fact_extraction_quality')\n",
    "    print(\"\\nüîç Fact Extraction Agreement:\")\n",
    "    for pair, scores in extraction_agreement.items():\n",
    "        print(f\"  {pair}: Œ∫={scores['kappa']:.3f} ({scores['interpretation']}) - {scores['items_compared']} items\")\n",
    "    \n",
    "    # Fact modification agreement  \n",
    "    modification_agreement = calculate_agreement_scores(news_annotations, 'fact_modification_quality')\n",
    "    print(\"\\nüîÑ Fact Modification Agreement:\")\n",
    "    for pair, scores in modification_agreement.items():\n",
    "        print(f\"  {pair}: Œ∫={scores['kappa']:.3f} ({scores['interpretation']}) - {scores['items_compared']} items\")\n",
    "\n",
    "if tweet_annotations:\n",
    "    print(\"\\nüìä TWEET EVALUATION AGREEMENT ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Similar analysis for tweets\n",
    "    extraction_agreement = calculate_agreement_scores(tweet_annotations, 'fact_extraction_quality')\n",
    "    print(\"\\nüîç Fact Extraction Agreement:\")\n",
    "    for pair, scores in extraction_agreement.items():\n",
    "        print(f\"  {pair}: Œ∫={scores['kappa']:.3f} ({scores['interpretation']}) - {scores['items_compared']} items\")\n",
    "    \n",
    "    modification_agreement = calculate_agreement_scores(tweet_annotations, 'fact_modification_quality')\n",
    "    print(\"\\nüîÑ Fact Modification Agreement:\")\n",
    "    for pair, scores in modification_agreement.items():\n",
    "        print(f\"  {pair}: Œ∫={scores['kappa']:.3f} ({scores['interpretation']}) - {scores['items_compared']} items\")\n",
    "\n",
    "# Overall quality assessment\n",
    "print(\"\\nüéØ QUALITY ASSESSMENT RECOMMENDATIONS:\")\n",
    "print(\"=\"*50)\n",
    "print(\"Based on agreement analysis:\")\n",
    "print(\"‚Ä¢ Œ∫ > 0.60: Proceed with full dataset processing\")\n",
    "print(\"‚Ä¢ Œ∫ 0.40-0.60: Review and improve fact schemas, then retest\")\n",
    "print(\"‚Ä¢ Œ∫ < 0.40: Significant improvements needed before scaling\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"Agreement analysis code ready - uncomment when annotations are complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19414575",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides:\n",
    "\n",
    "1. **Sample Creation**: Random sampling of generated results for evaluation\n",
    "2. **Annotation Templates**: Structured files for manual evaluation by 3+ annotators\n",
    "3. **Agreement Analysis**: Cohen's Kappa calculation for inter-annotator reliability\n",
    "4. **Quality Assessment**: Recommendations based on agreement scores\n",
    "\n",
    "### Next Steps:\n",
    "1. Run this notebook to create evaluation templates\n",
    "2. Distribute templates to 3+ annotators\n",
    "3. Collect completed annotations\n",
    "4. Run agreement analysis\n",
    "5. Decide whether to proceed with full dataset based on quality scores\n",
    "\n",
    "### Agreement Interpretation:\n",
    "- **Œ∫ > 0.60**: Good agreement - proceed with full processing\n",
    "- **Œ∫ 0.40-0.60**: Moderate agreement - consider improvements\n",
    "- **Œ∫ < 0.40**: Poor agreement - significant improvements needed"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
